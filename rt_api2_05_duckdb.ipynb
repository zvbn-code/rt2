{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abfrage Schnittstelle und Ablage in DuckDBzum Hafas Echtzeit-Archiv Produktiv / Demo-System / Ablage in Parquet\n",
    "\n",
    "Stand: 22.09.2024\n",
    "\n",
    "#### Aufgaben\n",
    "- Schema XML V14 Produktiv https://fahrplaner.vbn.de/archive/services/archiveExportService/v14?wsdl \n",
    "- Schema XML V15 demo https://vbn.demo.hafas.de/archive/services/archiveExportService/v15?wsdl\n",
    "- Dokumentation unter docs/\n",
    "- Einbauen Fahrt Start ende scheduleDepartureStation scheduleDepartureTime bzw. Arrival"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import os\n",
    "\n",
    "from sqlalchemy import create_engine #als Alternative zu Mysql pyscopg2 Connector\n",
    "from sqlalchemy import text\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import logging\n",
    "log_file = f\"log/log_rt.txt\"\n",
    "logging.basicConfig(filename=log_file, \n",
    "                        level=logging.INFO,\n",
    "                        style=\"{\",\n",
    "                        format=\"{asctime} [{levelname:8}] {message}\",\n",
    "                        datefmt=\"%d.%m.%Y %H:%M:%S\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Import xml gestartet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rt_archiv_func_08' from '/home/zvbn/python/rt2/rt_archiv_func_08.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys; sys.path.append('/home/zvbn/python/rt2')\n",
    "import para\n",
    "import rt_archiv_func_08 as rt_func #Import der benutzerdefinierten Funktionen\n",
    "reload(rt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\".env\")\n",
    "#config['CLIENT_ID_DEMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufrufen der SOAP-Abfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_xml(api_version, xml_request, xml_out, myUrl):\n",
    "    #Zugriff auf Hafas RT Archiv Produktiv System und Zugriffsschlüssel \n",
    "\n",
    "    req_ini = requests.post(myUrl, data=xml_request)\n",
    "    root = ET.fromstring(req_ini.text)\n",
    "    print(req_ini.text)\n",
    "    \n",
    "    #Ermitteln der Export ID\n",
    "    for child in root.iter('exportId'):\n",
    "        print(child.tag, child.attrib, child.text)\n",
    "        exportId = child.text\n",
    "    xml_status = f\"\"\"\n",
    "                <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" \n",
    "                    xmlns:v{api_version}=\"http://v{api_version}.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
    "               <soapenv:Header/>\n",
    "                    <soapenv:Body>\n",
    "                        <v{api_version}:getArchiveExportStatus>\n",
    "                            <exportId>{exportId}</exportId>\n",
    "                        </v{api_version}:getArchiveExportStatus>\n",
    "                    </soapenv:Body>\n",
    "              </soapenv:Envelope>\n",
    "              \"\"\"\n",
    "    #Abfragen und Warten auf Completed\n",
    "    status = ''\n",
    "    time.sleep(2) # initiales Warten auf Beendigung\n",
    "    while status != 'COMPLETED':\n",
    "        r = requests.post(myUrl, data=xml_status)\n",
    "        #print(r, '\\n',r.text)\n",
    "        root = ET.fromstring(r.text)\n",
    "        for child in root.iter('status'):\n",
    "            #print(child.tag, child.attrib, child.text)\n",
    "            status = child.text\n",
    "            print(f'{dt.datetime.now()} Status: {status}')\n",
    "            if status != 'COMPLETED': # Pause falls Job nicht beendet (Status nicht completed d.h. in process)\n",
    "                time.sleep(10) # Pause von 20 Sekunden bis zur nächsten Abfrage des Status\n",
    "    \n",
    "    # Afrage nach Beendigung Journey List\n",
    "\n",
    "    xml_jl = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "                 '<soapenv:Header/><soapenv:Body>'\n",
    "                    '<v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                       '<exportId>' + exportId + '</exportId>'              \n",
    "                     '</v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                 '</soapenv:Body>'\n",
    "          '</soapenv:Envelope>')\n",
    "    \n",
    "    rj = requests.post(myUrl, data=xml_jl)\n",
    "\n",
    "    #Ausgabe des Ergebnis XML Journey\n",
    "    dom = xml.dom.minidom.parseString(rj.text)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    \n",
    "    jl = open(os.path.join(xml_out), 'w')\n",
    "    print(pretty_xml_as_string, file = jl)\n",
    "    print(os.path.join(xml_out), 'gespeichert')\n",
    "\n",
    "    jl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import xml Fahrten > Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_rt_xml_to_df_fahrten(xml_file):\n",
    "    format_date = '%Y-%m-%dT%H:%M:%S'\n",
    "    lop = []\n",
    "    \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xml_file)\n",
    "    \n",
    "    # get root element \n",
    "    root = tree.getroot() \n",
    "\n",
    "    for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "        for journey in child.iter('journey'):\n",
    "\n",
    "            #Ermitteln der Feldinhalte\n",
    "            deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "            operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format_date).strftime('%Y-%m-%d')\n",
    "            fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "\n",
    "            deviceId = rt_func.isnone(journey.find('deviceId'))\n",
    "            clientId = rt_func.split_deviceid(journey.find('deviceId'))            \n",
    "\n",
    "            journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "            ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "            ex_linid_short = ':'.join(ex_lineid.split(':')[0:3])\n",
    "            lineshortname = rt_func.isnone(journey.find('lineShortName'))\n",
    "            destination = rt_func.isnone(journey.find('destination'))\n",
    "\n",
    "            hasRealtime = rt_func.isnone_boolean(journey.find('hasRealtime'))\n",
    "            journeyRtType = rt_func.isnone(journey.find('journeyRtType'))            \n",
    "\n",
    "            journeycancelled = rt_func.isnone(journey.find('journeyCancelled')).capitalize()\n",
    "            ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "            reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "            cancelled_kum = True if str(reported_cancelled) == 'True' else True if str(journeycancelled) == 'True' else False\n",
    "\n",
    "            #Ermitteln FahrtStartEnde\n",
    "            for sub in journey.iter('scheduleDepartureTime'):\n",
    "                fahrtstarttime = rt_func.isnone_delay(sub.find('scheduleTime'))\n",
    "            for sub in journey.iter('scheduleArrivalTime'):\n",
    "                fahrtendtime = rt_func.isnone_delay(sub.find('scheduleTime'))\n",
    "            for sub in journey.iter('scheduleDepartureStation'):\n",
    "                fahrtstartstationname = rt_func.isnone_delay(sub.find('stationName'))\n",
    "                fahrtstartstationdhid = rt_func.isnone_delay(sub.find('dhid'))\n",
    "            for sub in journey.iter('scheduleArrivalStation'):\n",
    "                fahrtendstationname = rt_func.isnone_delay(sub.find('stationName'))\n",
    "                fahrtendstationdhid = rt_func.isnone_delay(sub.find('dhid'))\n",
    "\n",
    "            \n",
    "            lop.append([operday, fnr, destination, hasRealtime, journeyOperator, ex_lineid, ex_linid_short, lineshortname, \\\n",
    "                        reported_cancelled, journeycancelled, ts_reported_cancelled, cancelled_kum, deviceId, clientId, journeyRtType, \\\n",
    "                            fahrtstarttime, fahrtstartstationname, fahrtstartstationdhid, fahrtendtime, fahrtendstationname, fahrtendstationdhid])\n",
    "            \n",
    "            child.clear()\n",
    "\n",
    "    df_fahrten = pd.DataFrame(lop, columns=['datum','fnr' ,'destination','hasRealtime' ,'vu', 'lineid', 'lineid_short', 'lineshort', \\\n",
    "                                            'reported_cancelled', 'journey_cancelled','ts_reported_cancelled' ,'cancelled_kum', 'deviceid', \\\n",
    "                                                'clientid', 'journeyrttype', 'fahrtstarttime', 'fahrtstartstationname', 'fahrtstartstationdhid',\\\n",
    "                                                      'fahrtendtime', 'fahrtendstationname', 'fahrtendstationdhid'])\n",
    "    return df_fahrten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import xml Verlauf > Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_rt_xml_to_df_verlauf(xml_file):\n",
    "    format_dt = '%Y-%m-%dT%H:%M:%S'\n",
    "    lop = []\n",
    "\n",
    "    # create element tree object \n",
    "    tree = ET.parse(xml_file)\n",
    "    \n",
    "    # get root element \n",
    "    root = tree.getroot() \n",
    "    for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "        for journey in child.iter('journey'):\n",
    "            has_rt = rt_func.isnone(journey.find('hasRealtime'))\n",
    "            \n",
    "            deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "            fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "            lineshortname = str(rt_func.isnone(journey.find('lineShortName'))).strip()\n",
    "            ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "            journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "            operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format_dt).strftime('%Y-%m-%d')\n",
    "            ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "            reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "\n",
    "        for details in child.iter('details'):\n",
    "            index = rt_func.isnone(details.find('index'))\n",
    "            for ddelay in details.iter('departureDelay'):\n",
    "                dep_del = rt_func.isnone_delay(ddelay.find('delay'))\n",
    "\n",
    "            for adelay in details.iter('arrivalDelay'):\n",
    "                arr_del = rt_func.isnone_delay(adelay.find('delay'))\n",
    "            \n",
    "            canc = rt_func.isnone(details.find('cancelled'))\n",
    "            \n",
    "            additional =  rt_func.isnone(details.find('additional'))\n",
    "\n",
    "            for station in details.iter('station'):\n",
    "                lat = int(station.find('latitude').text)/1000000\n",
    "                lon = int(station.find('longitude').text)/1000000\n",
    "                station_nr = station.find('stationExternalNumber').text\n",
    "                if station.find('stationName') is not None:\n",
    "                    station_name = station.find('stationName').text\n",
    "                else:\n",
    "                    station_name = '-'\n",
    "            \n",
    "            for dschedule in details.iter('scheduleDepartureTime'):\n",
    "                dschedtime= dschedule.find('scheduleTime')\n",
    "                if dschedtime is not None:\n",
    "                    dschedtime = dt.datetime.strptime(dschedtime.text[:-6], format_dt).strftime('%Y%m%d%H%M%S') #Umwandlung der Zeitformat da in 3.6 kein ISO-Format vorhanden\n",
    "                else:\n",
    "                    dschedtime =''\n",
    "            for aschedule in details.iter('scheduleArrivalTime'):\n",
    "                aschedtime = aschedule.find('scheduleTime')\n",
    "                if aschedtime is not None:\n",
    "                    aschedtime = dt.datetime.strptime(aschedtime.text[:-6], format_dt).strftime('%Y%m%d%H%M%S')\n",
    "                else: \n",
    "                    aschedtime =''\n",
    "\n",
    "            lop.append([operday, journeyOperator, deviceid, lineshortname, ex_lineid, \n",
    "                                    fnr, index, has_rt, dschedtime, aschedtime, dep_del, arr_del, station_nr, station_name, lat, lon, canc, additional, \n",
    "                                    ts_reported_cancelled, reported_cancelled])\n",
    "    \n",
    "    df_verlauf = pd.DataFrame(lop, columns=['operday','journeyOperator' ,'deviceid','lineshortname' ,'ex_lineid', 'fnr', 'index', 'has_rt', \n",
    "                                            'dschedtime', 'aschedtime','dep_del' ,'arr_del', 'station_nr', 'station_name', 'lat', 'lon', 'canc', 'additional', \n",
    "                                            'ts_reported_cancelled', 'reported_cancelled'])\n",
    "    return df_verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausgabe als formatiertes xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testen des XML mit schöner Ausgabe\n",
    "def print_pretty_xml(xml_request):\n",
    "    dom = xml.dom.minidom.parseString(xml_request)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    print(pretty_xml_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xml to tar.gz\n",
    "- Packen und Löschen des Ausgangs xml Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_targz(xml_path,xml_file):\n",
    "    \"\"\"Packen des xml-files\"\"\"\n",
    "    tar_gz = xml_file + '.tar.gz'\n",
    "\n",
    "    if os.path.exists(os.path.join(xml_path, tar_gz)):\n",
    "        with tarfile.open(os.path.join(xml_path, tar_gz), 'r:gz') as tar:\n",
    "            # Extract all files to the specified directory    \n",
    "            tar.extractall(xml_path)\n",
    "    else:\n",
    "        print('no tar.gz')\n",
    "\n",
    "    with tarfile.open(os.path.join(xml_path, tar_gz), 'w:gz') as archive:\n",
    "        # Add files to the tarball\n",
    "        archive.add(os.path.join(xml_path, xml_file), arcname= xml_file)\n",
    "                    \n",
    "    os.remove(os.path.join(xml_path, xml_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umwandlung der Datentypen Fahrten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_df_fahrten(df_rt_vbn_fahrten):\n",
    "    \"\"\"Umwandlung in verwendbare Boolean Typen\"\"\"\n",
    "    df_rt_vbn_fahrten['datum'] = pd.to_datetime(df_rt_vbn_fahrten['datum'], format='%Y-%m-%d')\n",
    "    #Umwandlung be gemischten Zeitzonen manuell mit strptime\n",
    "    #df_rt_vbn_fahrten['fahrtstarttime'] = pd.to_datetime(df_rt_vbn_fahrten['fahrtstarttime'], utc=True)\n",
    "    df_rt_vbn_fahrten['journey_cancelled'] = df_rt_vbn_fahrten['journey_cancelled'].replace({'True':True,'False':False},regex=True)\n",
    "    return df_rt_vbn_fahrten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umwandlung der Datentypen Verlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_df_verlauf(df_rt_vbn_verlauf):\n",
    "    \"\"\" Anpassung der verschiedenen Datentypen in der Datei Verlauf\"\"\"\n",
    "    df_rt_vbn_verlauf['lat'] = df_rt_vbn_verlauf['lat'].astype(float)\n",
    "    df_rt_vbn_verlauf['lon'] = df_rt_vbn_verlauf['lon'].astype(float)\n",
    "    df_rt_vbn_verlauf['dep_del'] = df_rt_vbn_verlauf['dep_del'].astype(float)\n",
    "    df_rt_vbn_verlauf['arr_del'] = df_rt_vbn_verlauf['arr_del'].astype(float)\n",
    "    df_rt_vbn_verlauf['canc'] = df_rt_vbn_verlauf['canc'].replace({'true':True,'false':False},regex=True)\n",
    "    df_rt_vbn_verlauf['has_rt'] = df_rt_vbn_verlauf['has_rt'].replace({'true':True,'false':False},regex=True)\n",
    "    df_rt_vbn_verlauf['additional'] = df_rt_vbn_verlauf['additional'].replace({'true':True,'false':False},regex=True)\n",
    "    df_rt_vbn_verlauf['reported_cancelled'] = df_rt_vbn_verlauf['reported_cancelled'].replace({'True':True,'False':False},regex=True)\n",
    "    df_rt_vbn_verlauf['index'] = df_rt_vbn_verlauf['index'].astype('Int32')\n",
    "    df_rt_vbn_verlauf['operday'] = pd.to_datetime(df_rt_vbn_verlauf['operday'], format='%Y-%m-%d')\n",
    "    df_rt_vbn_verlauf['dschedtime'] = pd.to_datetime(df_rt_vbn_verlauf['dschedtime'], format='%Y%m%d%H%M%S')\n",
    "    df_rt_vbn_verlauf['aschedtime'] = pd.to_datetime(df_rt_vbn_verlauf['aschedtime'], format='%Y%m%d%H%M%S')\n",
    "    return df_rt_vbn_verlauf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ermitteln verschiedener Zeitpunkte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jetzt = dt.datetime.now().strftime('%Y%m%d%H%M')\n",
    "heute = dt.date.today().strftime('%Y%m%d')\n",
    "heute_ll = dt.datetime.now().strftime('%d.%m.%Y %H:%M')\n",
    "gestern = (dt.date.today() - timedelta(1)).strftime('%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Linienliste / Zuordnung Bündel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen aus der lokalen DM Datenbank Wortmann Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung erfolgreich -lokale Datei aktualisiert\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{config['POSTGRES_USER']}:{config['POSTGRES_PW']}@127.0.0.1:5432/zvbn_postgis\")\n",
    "    #conn_dm = psycopg2.connect(database='zvbn_postgis', user='postgres', password=para.key_dm_db, host = '127.0.0.1')\n",
    "    sql_lin = \"\"\"SELECT nummer AS linie, buendel, \\'\\' AS rt_operator, ebene, dlid, id \n",
    "        FROM basis.linien \n",
    "        WHERE buendel IS NOT NULL AND aktiv IS TRUE \n",
    "        ORDER BY buendel, ebene, nummer \"\"\"\n",
    "    sql_buendel = 'SELECT * FROM basis.lin_buendel'\n",
    "    df_lin_dm =  pd.read_sql(text(sql_lin), engine.connect())\n",
    "    df_buendel = pd.read_sql(text(sql_buendel), engine.connect())\n",
    "    df_lin_dm.to_csv('input/linien_dm.csv', sep=';', index=False)\n",
    "    print('Verbindung erfolgreich -lokale Datei aktualisiert')\n",
    "except:\n",
    "    df_lin_dm = pd.read_csv('input/linien_dm.csv', sep=';') #aktuelle Zuordnung Linie zu Bündel aus DM\n",
    "    print(f'Verbindung nicht erfolgreich - Verwendung lokale Datei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abruf XML und Erstellen Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesamt VBN\n",
    "\n",
    "- Abfagen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4: und 910 aus Cloppenburg\n",
    "- lineExternalNamePattern Abfrage über DLID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen der Abfrage für xml-Soap mit Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_xml_request_dlid(start, ende, api_version, clientID, matrix, lineExternalNamePattern):\n",
    "     \"\"\" Erstellen der SOAP Abfrage mit verschiedenen Parametern\"\"\"\n",
    "     if api_version >= 15:\n",
    "        options = f\"\"\"\n",
    "               <options>\n",
    "                    <includeMatrixData>{str(matrix).lower()}</includeMatrixData>\n",
    "               </options>\n",
    "               \"\"\"\n",
    "     else:\n",
    "        options = \"\"\n",
    "     \n",
    "     xml_request_dlid = f\"\"\"\n",
    "     <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v{api_version}=\"http://v{api_version}.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
    "                    <soapenv:Header/>\n",
    "                    <soapenv:Body>\n",
    "                    <v{api_version}:createArchiveJob>\n",
    "                         <filter>\n",
    "                              <clientId>{clientID}</clientId>                    \n",
    "                              <startDate>{start}</startDate>\n",
    "                              <endDate>{ende}</endDate>\n",
    "                              <lineExternalNamePattern>{lineExternalNamePattern}</lineExternalNamePattern>            \n",
    "                              <hasRealtime>ALL</hasRealtime>\n",
    "                         </filter>\n",
    "                         {options}\n",
    "                    </v{api_version}:createArchiveJob>\n",
    "                    \n",
    "               </soapenv:Body>\n",
    "          </soapenv:Envelope>\n",
    "                    \"\"\"\n",
    "     return xml_request_dlid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "                    <soapenv:Header/>\n",
      "                    <soapenv:Body>\n",
      "                    <v14:createArchiveJob>\n",
      "                         <filter>\n",
      "                              <clientId>PMQmY5p9y8kmoTno</clientId>                    \n",
      "                              <startDate>2024-09-21</startDate>\n",
      "                              <endDate>2024-09-21</endDate>\n",
      "                              <lineExternalNamePattern>xx</lineExternalNamePattern>            \n",
      "                              <hasRealtime>ALL</hasRealtime>\n",
      "                         </filter>\n",
      "                         \n",
      "                    </v14:createArchiveJob>\n",
      "                    \n",
      "               </soapenv:Body>\n",
      "          </soapenv:Envelope>\n",
      "                    \n"
     ]
    }
   ],
   "source": [
    "print(def_xml_request_dlid(start=gestern, ende=gestern, api_version=14, clientID=config['CLIENT_ID_PROD'], matrix=False, lineExternalNamePattern='xx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen der Abfrage für xml-Soap mit Funktion Zusatzfahrten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_xml_request_zusatz(start, ende, api_version, clientID):\n",
    "        \"\"\"Erstellen der SOAP-Anfrage für den Teil Zusatzfahrten\"\"\"\n",
    "        xml_request_zusatz_umleitung = f\"\"\"\n",
    "                                    <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" \n",
    "                xmlns:v{api_version}=\"http://v{api_version}.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
    "                <soapenv:Header/><soapenv:Body><v{api_version}:createArchiveJob>\n",
    "                <filter>\n",
    "                        <clientId>{clientID}</clientId>         \n",
    "                        <startDate>{start}</startDate>\n",
    "                        <endDate>{ende}</endDate>\n",
    "                        <filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>\n",
    "                        <filterJourneyRtTypeList>REALTIME_EXTRA_REPLACEMENT</filterJourneyRtTypeList>\n",
    "                        <filterJourneyRtTypeList>REALTIME_EXTRA_REPORTED</filterJourneyRtTypeList>\n",
    "                        <filterJourneyRtTypeList>REALTIME_EXTRA_MAINTENANCE</filterJourneyRtTypeList>\n",
    "                        <filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>\n",
    "                        <filterJourneyRtTypeList>DEVIATION_OF_REALTIME_EXTRA</filterJourneyRtTypeList>         \n",
    "                        <filterJourneyRtTypeList>DEVIATION_OF_REPLACEMENT</filterJourneyRtTypeList>             \n",
    "                        <filterJourneyRtTypeList>SUPPLEMENTARY</filterJourneyRtTypeList>'                \n",
    "                        <filterJourneyRtTypeList>UNKNOWN</filterJourneyRtTypeList>               \n",
    "                        <hasRealtime>ALL</hasRealtime>\n",
    "                </filter>\n",
    "                </v{api_version}:createArchiveJob></soapenv:Body></soapenv:Envelope>\n",
    "                \"\"\"\n",
    "        return xml_request_zusatz_umleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOAP Abfrage ausführen Verlauf / Fahrten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produktivsystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "                    <soapenv:Header/>\n",
      "                    <soapenv:Body>\n",
      "                    <v14:createArchiveJob>\n",
      "                         <filter>\n",
      "                              <clientId>PMQmY5p9y8kmoTno</clientId>                    \n",
      "                              <startDate>2024-09-21</startDate>\n",
      "                              <endDate>2024-09-21</endDate>\n",
      "                              <lineExternalNamePattern>de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:,de:VBN-VGC:910:</lineExternalNamePattern>            \n",
      "                              <hasRealtime>ALL</hasRealtime>\n",
      "                         </filter>\n",
      "                         \n",
      "                    </v14:createArchiveJob>\n",
      "                    \n",
      "               </soapenv:Body>\n",
      "          </soapenv:Envelope>\n",
      "                    \n",
      "no tar.gz\n",
      "<soap:Envelope xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"><soap:Body><ns2:createArchiveJobResponse xmlns:ns2=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\"><return><clientId>PMQmY5p9y8kmoTno</clientId><exportId>7044ef90-5c8f-4f47-aa46-2d0cd9dc7011</exportId><status>NEW</status><type>ARCHIVE_EXPORT</type></return></ns2:createArchiveJobResponse></soap:Body></soap:Envelope>\n",
      "exportId {} 7044ef90-5c8f-4f47-aa46-2d0cd9dc7011\n",
      "2024-09-22 17:12:08.817921 Status: NEW\n",
      "2024-09-22 17:12:18.995054 Status: IN_PROCESS\n",
      "2024-09-22 17:12:29.171803 Status: IN_PROCESS\n",
      "2024-09-22 17:12:39.361489 Status: IN_PROCESS\n",
      "2024-09-22 17:12:49.538617 Status: IN_PROCESS\n",
      "2024-09-22 17:12:59.725209 Status: IN_PROCESS\n",
      "2024-09-22 17:13:09.906569 Status: IN_PROCESS\n",
      "2024-09-22 17:13:20.100013 Status: IN_PROCESS\n",
      "2024-09-22 17:13:30.295117 Status: IN_PROCESS\n",
      "2024-09-22 17:13:40.465810 Status: IN_PROCESS\n",
      "2024-09-22 17:13:50.657794 Status: IN_PROCESS\n",
      "2024-09-22 17:14:00.834917 Status: IN_PROCESS\n",
      "2024-09-22 17:14:11.013875 Status: IN_PROCESS\n",
      "2024-09-22 17:14:21.209096 Status: IN_PROCESS\n",
      "2024-09-22 17:14:31.420379 Status: IN_PROCESS\n",
      "2024-09-22 17:14:41.585561 Status: IN_PROCESS\n",
      "2024-09-22 17:14:51.789639 Status: IN_PROCESS\n",
      "2024-09-22 17:15:02.112461 Status: IN_PROCESS\n",
      "2024-09-22 17:15:12.315599 Status: IN_PROCESS\n",
      "2024-09-22 17:15:22.501327 Status: IN_PROCESS\n",
      "2024-09-22 17:15:32.662998 Status: IN_PROCESS\n",
      "2024-09-22 17:15:42.848695 Status: IN_PROCESS\n",
      "2024-09-22 17:15:53.021394 Status: IN_PROCESS\n",
      "2024-09-22 17:16:03.215045 Status: IN_PROCESS\n",
      "2024-09-22 17:16:13.424054 Status: IN_PROCESS\n",
      "2024-09-22 17:16:23.616935 Status: IN_PROCESS\n",
      "2024-09-22 17:16:33.819831 Status: IN_PROCESS\n",
      "2024-09-22 17:16:44.008224 Status: IN_PROCESS\n",
      "2024-09-22 17:16:54.175323 Status: IN_PROCESS\n",
      "2024-09-22 17:17:04.350046 Status: IN_PROCESS\n",
      "2024-09-22 17:17:14.512099 Status: IN_PROCESS\n",
      "2024-09-22 17:17:24.711878 Status: IN_PROCESS\n",
      "2024-09-22 17:17:34.899706 Status: IN_PROCESS\n",
      "2024-09-22 17:17:45.077356 Status: IN_PROCESS\n",
      "2024-09-22 17:17:55.295125 Status: IN_PROCESS\n",
      "2024-09-22 17:18:05.485891 Status: IN_PROCESS\n",
      "2024-09-22 17:18:15.669816 Status: IN_PROCESS\n",
      "2024-09-22 17:18:25.850786 Status: IN_PROCESS\n",
      "2024-09-22 17:18:36.039580 Status: IN_PROCESS\n",
      "2024-09-22 17:18:46.225343 Status: IN_PROCESS\n",
      "2024-09-22 17:18:56.410647 Status: IN_PROCESS\n",
      "2024-09-22 17:19:06.596229 Status: IN_PROCESS\n",
      "2024-09-22 17:19:16.777622 Status: IN_PROCESS\n",
      "2024-09-22 17:19:26.971960 Status: IN_PROCESS\n",
      "2024-09-22 17:19:37.159862 Status: IN_PROCESS\n",
      "2024-09-22 17:19:47.348413 Status: IN_PROCESS\n",
      "2024-09-22 17:19:57.564352 Status: COMPLETED\n",
      "api_xml/prod/rt_archiv_14_2024-09-21_2024-09-21_alle_prod_matrix_False.xml gespeichert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1597631/2963147006.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_rt_vbn_verlauf['canc'] = df_rt_vbn_verlauf['canc'].replace({'true':True,'false':False},regex=True)\n",
      "/tmp/ipykernel_1597631/2963147006.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_rt_vbn_verlauf['has_rt'] = df_rt_vbn_verlauf['has_rt'].replace({'true':True,'false':False},regex=True)\n",
      "/tmp/ipykernel_1597631/2963147006.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_rt_vbn_verlauf['additional'] = df_rt_vbn_verlauf['additional'].replace({'true':True,'false':False},regex=True)\n",
      "/tmp/ipykernel_1597631/1951250727.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_rt_vbn_fahrten['journey_cancelled'] = df_rt_vbn_fahrten['journey_cancelled'].replace({'True':True,'False':False},regex=True)\n"
     ]
    }
   ],
   "source": [
    "start = gestern\n",
    "ende = gestern\n",
    "api_version = 14\n",
    "clientID = config['CLIENT_ID_PROD']\n",
    "server = 'prod' #prod oder demo\n",
    "#lineExternalNamePattern = 'de:VBN:680:*' #Auswahl\n",
    "lineExternalNamePattern = 'de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:,de:VBN-VGC:910:' #Gesamt VBN\n",
    "\n",
    "#Festlegen Prod oder Demosystem\n",
    "if server == 'prod':\n",
    "    clientID = config['CLIENT_ID_PROD'] #prod\n",
    "    api_version = 14\n",
    "    matrix = False #ab Version 15 true möglich\n",
    "    myUrl = f\"https://fahrplaner.vbn.de/archive/services/archiveExportService/v{api_version}?wsdl\"\n",
    "else:\n",
    "    clientID = config['CLIENT_ID_DEMO'] #demo\n",
    "    api_version = 15\n",
    "    matrix = True #ab Version 15\n",
    "    myUrl = f\"https://vbn.demo.hafas.de/archive/services/archiveExportService/v{api_version}?wsdl\"\n",
    "\n",
    "xml_request_dlid = def_xml_request_dlid(start=gestern, ende=gestern, api_version=api_version, clientID=clientID, matrix=matrix, lineExternalNamePattern=lineExternalNamePattern)\n",
    "print(xml_request_dlid)\n",
    "\n",
    "xml_path_pre = 'api_xml'\n",
    "\n",
    "xml_file = f\"rt_archiv_{api_version}_{start}_{ende}_alle_{server}_matrix_{matrix}.xml\"\n",
    "xml_path = os.path.join(xml_path_pre, server)\n",
    "xml_out = os.path.join(xml_path_pre, server, xml_file)\n",
    "tar_gz = f\"{xml_out}.tar.gz\"\n",
    "\n",
    "\n",
    "if os.path.exists(os.path.join(xml_path, tar_gz)):\n",
    "    with tarfile.open(os.path.join(xml_path, tar_gz), 'r:gz') as tar:\n",
    "        # Extract all files to the specified directory    \n",
    "        tar.extractall(xml_path) \n",
    "else:\n",
    "    print('no tar.gz')   \n",
    "\n",
    "request_xml(api_version=api_version, xml_request=xml_request_dlid, xml_out=xml_out, myUrl=myUrl)\n",
    "df_rt_vbn_fahrten = import_rt_xml_to_df_fahrten(xml_out)\n",
    "df_rt_vbn_verlauf = import_rt_xml_to_df_verlauf(xml_out)\n",
    "\n",
    "df_rt_vbn_verlauf = type_df_verlauf(df_rt_vbn_verlauf)\n",
    "df_rt_vbn_fahrten = type_df_fahrten(df_rt_vbn_fahrten)\n",
    "\n",
    "xml_to_targz(xml_file=xml_file, xml_path=xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"Anzahl Fahrten importiert {df_rt_vbn_fahrten.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2024-09-21\n",
       "Name: operday, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rt_vbn_verlauf.operday.drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Für Zusatzfahrten\n",
    "\n",
    "- Abfragen aller Daten über die RTTypes\n",
    "    - REALTIME_EXTRA und weitere\n",
    "    - DEVIATION_OF_SCHEDULED\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_request_zusatz_umleitung = f\"\"\"\n",
    "                                <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" \n",
    "               xmlns:v{api_version}=\"http://v{api_version}.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
    "               <soapenv:Header/><soapenv:Body><v{api_version}:createArchiveJob>\n",
    "               <filter>\n",
    "                    <clientId>{clientID}</clientId>         \n",
    "                    <startDate>{gestern}</startDate>\n",
    "                    <endDate>{gestern}</endDate>\n",
    "                    <filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>\n",
    "                    <filterJourneyRtTypeList>REALTIME_EXTRA_REPLACEMENT</filterJourneyRtTypeList>\n",
    "                    <filterJourneyRtTypeList>REALTIME_EXTRA_REPORTED</filterJourneyRtTypeList>\n",
    "                    <filterJourneyRtTypeList>REALTIME_EXTRA_MAINTENANCE</filterJourneyRtTypeList>\n",
    "                    <filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>\n",
    "                    <filterJourneyRtTypeList>DEVIATION_OF_REALTIME_EXTRA</filterJourneyRtTypeList>         \n",
    "                    <filterJourneyRtTypeList>DEVIATION_OF_REPLACEMENT</filterJourneyRtTypeList>             \n",
    "                    <filterJourneyRtTypeList>SUPPLEMENTARY</filterJourneyRtTypeList>'                \n",
    "                    <filterJourneyRtTypeList>UNKNOWN</filterJourneyRtTypeList>               \n",
    "                    <hasRealtime>ALL</hasRealtime>\n",
    "               </filter>\n",
    "               </v{api_version}:createArchiveJob></soapenv:Body></soapenv:Envelope>\n",
    "               \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "\t\n",
      "               \n",
      "\t<soapenv:Header/>\n",
      "\t<soapenv:Body>\n",
      "\t\t<v14:createArchiveJob>\n",
      "\t\t\t\n",
      "               \n",
      "\t\t\t<filter>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<clientId>PMQmY5p9y8kmoTno</clientId>\n",
      "\t\t\t\t         \n",
      "                    \n",
      "\t\t\t\t<startDate>2024-09-21</startDate>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<endDate>2024-09-21</endDate>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_REPLACEMENT</filterJourneyRtTypeList>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_REPORTED</filterJourneyRtTypeList>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_MAINTENANCE</filterJourneyRtTypeList>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>\n",
      "\t\t\t\t\n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_REALTIME_EXTRA</filterJourneyRtTypeList>\n",
      "\t\t\t\t         \n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_REPLACEMENT</filterJourneyRtTypeList>\n",
      "\t\t\t\t             \n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>SUPPLEMENTARY</filterJourneyRtTypeList>\n",
      "\t\t\t\t'                \n",
      "                    \n",
      "\t\t\t\t<filterJourneyRtTypeList>UNKNOWN</filterJourneyRtTypeList>\n",
      "\t\t\t\t               \n",
      "                    \n",
      "\t\t\t\t<hasRealtime>ALL</hasRealtime>\n",
      "\t\t\t\t\n",
      "               \n",
      "\t\t\t</filter>\n",
      "\t\t\t\n",
      "               \n",
      "\t\t</v14:createArchiveJob>\n",
      "\t</soapenv:Body>\n",
      "</soapenv:Envelope>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_pretty_xml(xml_request_zusatz_umleitung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<soap:Envelope xmlns:soap=\"http://schemas.xmlsoap.org/soap/envelope/\"><soap:Body><ns2:createArchiveJobResponse xmlns:ns2=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\"><return><clientId>PMQmY5p9y8kmoTno</clientId><exportId>21431a87-e5fa-4ac6-ad3e-2d93046fb03c</exportId><status>NEW</status><type>ARCHIVE_EXPORT</type></return></ns2:createArchiveJobResponse></soap:Body></soap:Envelope>\n",
      "exportId {} 21431a87-e5fa-4ac6-ad3e-2d93046fb03c\n",
      "2024-09-22 17:23:32.554682 Status: IN_PROCESS\n",
      "2024-09-22 17:23:42.736844 Status: IN_PROCESS\n",
      "2024-09-22 17:23:52.913942 Status: IN_PROCESS\n",
      "2024-09-22 17:24:03.100441 Status: IN_PROCESS\n",
      "2024-09-22 17:24:13.287496 Status: IN_PROCESS\n",
      "2024-09-22 17:24:23.457409 Status: IN_PROCESS\n",
      "2024-09-22 17:24:33.646211 Status: IN_PROCESS\n",
      "2024-09-22 17:24:43.830856 Status: IN_PROCESS\n",
      "2024-09-22 17:24:54.022102 Status: IN_PROCESS\n",
      "2024-09-22 17:25:04.217512 Status: IN_PROCESS\n",
      "2024-09-22 17:25:14.406679 Status: IN_PROCESS\n",
      "2024-09-22 17:25:24.593805 Status: IN_PROCESS\n",
      "2024-09-22 17:25:34.776100 Status: IN_PROCESS\n",
      "2024-09-22 17:25:44.975429 Status: IN_PROCESS\n",
      "2024-09-22 17:25:55.159165 Status: IN_PROCESS\n",
      "2024-09-22 17:26:05.348979 Status: IN_PROCESS\n",
      "2024-09-22 17:26:15.547515 Status: IN_PROCESS\n",
      "2024-09-22 17:26:25.743570 Status: IN_PROCESS\n",
      "2024-09-22 17:26:35.912833 Status: IN_PROCESS\n",
      "2024-09-22 17:26:46.110657 Status: IN_PROCESS\n",
      "2024-09-22 17:26:56.294543 Status: IN_PROCESS\n",
      "2024-09-22 17:27:06.476235 Status: IN_PROCESS\n",
      "2024-09-22 17:27:16.661608 Status: IN_PROCESS\n",
      "2024-09-22 17:27:26.833624 Status: COMPLETED\n",
      "api_xml/prod/rt_archiv_2024-09-21_zusatz.xml gespeichert\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "request_xml() got an unexpected keyword argument 'demosystem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m xml_out \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(xml_path_pre, server, xml_file)\n\u001b[1;32m     21\u001b[0m request_xml(api_version\u001b[38;5;241m=\u001b[39mapi_version, xml_request\u001b[38;5;241m=\u001b[39mxml_request_zusatz_umleitung, xml_out\u001b[38;5;241m=\u001b[39mxml_out, myUrl\u001b[38;5;241m=\u001b[39mmyUrl)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mrequest_xml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_request_zusatz_umleitung\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxml_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxml_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemosystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m df_rt_zusatz \u001b[38;5;241m=\u001b[39m import_rt_xml_to_df_fahrten(xml_file\u001b[38;5;241m=\u001b[39mxml_out)\n\u001b[1;32m     26\u001b[0m xml_to_targz(xml_file\u001b[38;5;241m=\u001b[39mxml_file, xml_path\u001b[38;5;241m=\u001b[39mxml_path)\n",
      "\u001b[0;31mTypeError\u001b[0m: request_xml() got an unexpected keyword argument 'demosystem'"
     ]
    }
   ],
   "source": [
    "xml_path_pre = 'api_xml'\n",
    "server = 'prod'\n",
    "xml_file = f\"rt_archiv_{gestern}_zusatz.xml\"\n",
    "api_version = 14\n",
    "\n",
    "#Festlegen Prod oder Demosystem\n",
    "if server == 'prod':\n",
    "    clientID = config['CLIENT_ID_PROD'] #prod\n",
    "    api_version = 14\n",
    "    matrix = False #ab Version 15 true möglich\n",
    "    myUrl = f\"https://fahrplaner.vbn.de/archive/services/archiveExportService/v{api_version}?wsdl\"\n",
    "else:\n",
    "    clientID = config['CLIENT_ID_DEMO'] #demo\n",
    "    api_version = 15\n",
    "    matrix = True #ab Version 15\n",
    "    myUrl = f\"https://vbn.demo.hafas.de/archive/services/archiveExportService/v{api_version}?wsdl\"\n",
    "\n",
    "xml_path = os.path.join(xml_path_pre, server)\n",
    "xml_out = os.path.join(xml_path_pre, server, xml_file)\n",
    "\n",
    "request_xml(api_version=api_version, xml_request=xml_request_zusatz_umleitung, xml_out=xml_out, myUrl=myUrl)\n",
    "\n",
    "df_rt_zusatz = import_rt_xml_to_df_fahrten(xml_file=xml_out)\n",
    "\n",
    "xml_to_targz(xml_file=xml_file, xml_path=xml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schreiben der Daten nach Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt_vbn_fahrten.to_parquet(f\"out/parquet/{server}/fahrten_{gestern.replace('-', '_')}.parquet\")\n",
    "df_rt_zusatz.to_parquet(f\"out/parquet/{server}/zusatz_{gestern.replace('-', '_')}.parquet\")\n",
    "df_rt_vbn_verlauf.to_parquet(f\"out/parquet/{server}/verlauf_{gestern.replace('-', '_')}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
