{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abfrage Schnittstelle und Ablage in DuckDBzum Hafas Echtzeit-Archiv Produktiv / Demo-System / Ablage in DuckDB\n",
    "\n",
    "Stand: 18.11.2023\n",
    "\n",
    "#### Aufgaben\n",
    "- Schema XML https://fahrplaner.vbn.de/archive/services/archiveExportService/v14?wsdl \n",
    "- Dokumentation unter docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "import datetime as dt\n",
    "import time\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import duckdb\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from sqlalchemy import create_engine #als Alternative zu Mysql pyscopg2 Connector\n",
    "from sqlalchemy import text\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rt_archiv_func_07' from '/home/zvbn/python/rt2/rt_archiv_func_07.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import para\n",
    "import rt_archiv_func_07 as rt_func #Import der benutzerdefinierten Funktionen\n",
    "reload(rt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen der Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufrufen der Abfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_xml(api_version, xml_request, xml_out, xml_path):\n",
    "    #Zugriff auf Hafas RT Archiv Produktiv System und Zugriffsschlüssel\n",
    "    myUrl = 'https://fahrplaner.vbn.de/archive/services/archiveExportService/v'+str(api_version)+'?wsdl'\n",
    "    clientID = 'PMQmY5p9y8kmoTno'\n",
    "    req_ini = requests.post(myUrl, data=xml_request)\n",
    "    root = ET.fromstring(req_ini.text)\n",
    "    #Ermitteln der Export ID\n",
    "    for child in root.iter('exportId'):\n",
    "        #print(child.tag, child.attrib, child.text)\n",
    "        exportId = child.text\n",
    "    xml_status = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/>'\n",
    "                    '<soapenv:Body>'\n",
    "                        '<v'+str(api_version)+':getArchiveExportStatus>'\n",
    "                            '<exportId>' + exportId + '</exportId>'\n",
    "                        '</v'+str(api_version)+':getArchiveExportStatus>'\n",
    "                    '</soapenv:Body>'\n",
    "              '</soapenv:Envelope>')\n",
    "    #Abfragen und Warten auf Completed\n",
    "    status = ''\n",
    "    time.sleep(2) # initiales Warten auf Beendigung\n",
    "    while status != 'COMPLETED':\n",
    "        r = requests.post(myUrl, data=xml_status)\n",
    "        #print(r, '\\n',r.text)\n",
    "        root = ET.fromstring(r.text)\n",
    "        for child in root.iter('status'):\n",
    "            #print(child.tag, child.attrib, child.text)\n",
    "            status = child.text\n",
    "            print(f'{dt.datetime.now()} Status: {status}')\n",
    "            if status != 'COMPLETED': # Pause falls Job nicht beendet (Status nicht completed d.h. in process)\n",
    "                time.sleep(20) # Pause von 20 Sekunden bis zur nächsten Abfrage des Status\n",
    "    \n",
    "    # Afrage nach Beendigung\n",
    "    xml_jl = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "                 '<soapenv:Header/><soapenv:Body>'\n",
    "                    '<v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                       '<exportId>' + exportId + '</exportId>'              \n",
    "                     '</v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                 '</soapenv:Body>'\n",
    "          '</soapenv:Envelope>')\n",
    "    \n",
    "    r = requests.post(myUrl, data=xml_jl)\n",
    "\n",
    "    #Ausgabe des Ergebnis XML\n",
    "    dom = xml.dom.minidom.parseString(r.text)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    \n",
    "    jl = open(os.path.join(xml_path, xml_out), 'w')\n",
    "    print(pretty_xml_as_string, file = jl)\n",
    "    print(os.path.join(xml_path, xml_out), 'gespeichert')\n",
    "\n",
    "    jl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des xml und Umwandeln nach Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_rt_xml_to_df(xml_file):\n",
    "    format_date = '%Y-%m-%dT%H:%M:%S'\n",
    "    lop = []\n",
    "    \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xml_file)\n",
    "    \n",
    "    # get root element \n",
    "    root = tree.getroot() \n",
    "\n",
    "    for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "        for journey in child.iter('journey'):\n",
    "\n",
    "            #Ermitteln der Feldinhalte\n",
    "            operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format_date).strftime('%Y-%m-%d')\n",
    "            fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "\n",
    "            deviceId = rt_func.isnone(journey.find('deviceId'))\n",
    "            clientId = rt_func.split_deviceid(journey.find('deviceId'))            \n",
    "\n",
    "            journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "            ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "            ex_linid_short = ':'.join(ex_lineid.split(':')[0:3])\n",
    "            lineshortname = rt_func.isnone(journey.find('lineShortName'))\n",
    "            destination = rt_func.isnone(journey.find('destination'))\n",
    "\n",
    "            hasRealtime = rt_func.isnone_boolean(journey.find('hasRealtime'))\n",
    "            journeyRtType = rt_func.isnone(journey.find('journeyRtType'))            \n",
    "\n",
    "            journeycancelled = rt_func.isnone(journey.find('journeyCancelled')).capitalize()\n",
    "            ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "            reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "            cancelled_kum = True if str(reported_cancelled) == 'True' else True if str(journeycancelled) == 'True' else False\n",
    "            \n",
    "            lop.append([operday, fnr, destination, hasRealtime, journeyOperator, ex_lineid, ex_linid_short, lineshortname, \\\n",
    "                        reported_cancelled, journeycancelled, ts_reported_cancelled, cancelled_kum, deviceId, clientId, journeyRtType])\n",
    "            \n",
    "            child.clear()\n",
    "\n",
    "    df_fahrten = pd.DataFrame(lop, columns=['datum','fnr' ,'destination','hasRealtime' ,'vu', 'lineid', 'lineid_short', 'lineshort', \\\n",
    "                                            'reported_cancelled', 'journey_cancelled','ts_reported_cancelled' ,'cancelled_kum', 'deviceid', 'clientid', 'journeyrttype' ])\n",
    "    return df_fahrten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausgabe als formatiertes xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testen des XML mit schöner Ausgabe\n",
    "def print_pretty_xml(xml_request):\n",
    "    dom = xml.dom.minidom.parseString(xml_request)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    print(pretty_xml_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xml to tar.gz\n",
    "- Packen und Löschen des Ausgangs xml Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_targz(xml_path, xml_file):\n",
    "    tar_gz = xml_file + '.tar.gz'\n",
    "\n",
    "    if os.path.exists(os.path.join(xml_path, tar_gz)):\n",
    "        with tarfile.open(os.path.join(xml_path, tar_gz), 'r:gz') as tar:\n",
    "            # Extract all files to the specified directory    \n",
    "            tar.extractall(xml_path)\n",
    "    else:\n",
    "        print('no tar.gz')\n",
    "\n",
    "    with tarfile.open(os.path.join(xml_path, tar_gz), 'w:gz') as archive:\n",
    "        # Add files to the tarball\n",
    "        archive.add(os.path.join(xml_path, xml_file), arcname= xml_file)\n",
    "                    \n",
    "    os.remove(os.path.join(xml_path, xml_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ermitteln verschiedener Zeitpunkte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heute: 20231204 \n",
      "Jetzt: 202312040821 \n",
      "Gestern: 2023-12-03 \n",
      "Vorgestern: 2023-12-02 \n",
      "Vor vier Wochen: 2023-11-06 \n",
      "Vor sechs Wochen: 2023-10-23\n",
      "vor einer Woche 2023-11-27\n",
      "Erster Tag des letzen Monats: 2023-11-01 und letzter Tag:  2023-11-30\n"
     ]
    }
   ],
   "source": [
    "#Ermitteln der Zeitstempel\n",
    "jetzt = dt.datetime.now().strftime('%Y%m%d%H%M')\n",
    "heute = dt.date.today().strftime('%Y%m%d')\n",
    "heute_ll = dt.datetime.now().strftime('%d.%m.%Y %H:%M')\n",
    "gestern = (dt.date.today() - timedelta(1)).strftime('%Y-%m-%d')\n",
    "vorgestern = (dt.datetime.now() - timedelta(2)).strftime('%Y-%m-%d')\n",
    "vorvierwochen = (dt.date.today() - timedelta(28)).strftime('%Y-%m-%d')\n",
    "vorsechswochen = (dt.date.today() - timedelta(42)).strftime('%Y-%m-%d')\n",
    "letztesiebentage = (dt.date.today() - timedelta(7)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'Heute: {heute} \\nJetzt: {jetzt} \\nGestern: {gestern} \\nVorgestern: {vorgestern} \\nVor vier Wochen: {vorvierwochen} \\nVor sechs Wochen: {vorsechswochen}')\n",
    "print('vor einer Woche ' + letztesiebentage)\n",
    "#ermitteln des letzten Monats\n",
    "\n",
    "today = dt.date.today()\n",
    "first = today.replace(day=1) #auf den ersten des aktuellen Monats setzen\n",
    "lastmonth_last = first - dt.timedelta(days=1) # 1 Tag abziehen vom ersten Tag \n",
    "lastmonth_first = lastmonth_last.replace(day=1) #ersetzen des ersten Tages\n",
    "lastmonth_first_str = str(lastmonth_first)\n",
    "lastmonth_last_str = str(lastmonth_last)\n",
    "print(f'Erster Tag des letzen Monats: {lastmonth_first} und letzter Tag:  {lastmonth_last}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aktuelle Version der Schnittstellenbeschreibung unter \n",
    "- docs/SmartVMS Export API v12.pdf\n",
    "- docs/SmartVMS Export API v14_datatypes-1.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = 14 #14 auf demo und ab August 2023 auf prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zugriff auf Hafas RT Archiv Produktiv System und Zugriffsschlüssel\n",
    "myUrl = 'https://fahrplaner.vbn.de/archive/services/archiveExportService/v'+str(api_version)+'?wsdl'\n",
    "clientID = 'PMQmY5p9y8kmoTno'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Linienliste / Zuordnung Bündel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen aus der lokalen DM Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung erfolgreich -lokale Datei aktualisiert\n"
     ]
    }
   ],
   "source": [
    "#Zugriff auf die lokale Datenbank auf dem Wortmann Debian Server\n",
    "\n",
    "try:\n",
    "    engine = create_engine(\"postgresql+psycopg2://postgres:\"+para.key_dm_db+\"@127.0.0.1:5432/zvbn_postgis\")\n",
    "    #conn_dm = psycopg2.connect(database='zvbn_postgis', user='postgres', password=para.key_dm_db, host = '127.0.0.1')\n",
    "    sql_lin = 'SELECT nummer AS linie, buendel, \\'\\' AS rt_operator, ebene, dlid, id \\\n",
    "        FROM basis.linien \\\n",
    "        WHERE buendel IS NOT NULL AND aktiv IS TRUE \\\n",
    "        ORDER BY buendel, ebene, nummer'\n",
    "    sql_buendel = 'SELECT * FROM basis.lin_buendel'\n",
    "    df_lin_dm =  pd.read_sql(text(sql_lin), engine.connect())\n",
    "    df_buendel = pd.read_sql(text(sql_buendel), engine.connect())\n",
    "    df_lin_dm.to_csv('input/linien_dm.csv', sep=';', index=False)\n",
    "    print('Verbindung erfolgreich -lokale Datei aktualisiert')\n",
    "except:\n",
    "    df_lin_dm = pd.read_csv('input/linien_dm.csv', sep=';') #aktuelle Zuordnung Linie zu Bündel aus DM\n",
    "    print(f'Verbindung nicht erfolgreich - Verwendung lokale Datei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Linids, die nicht mit de:VBN starten\n",
    "#df_lin_dm.fillna('-').query('not (dlid.str.startswith(\"de:VBN\") or dlid.str.startswith(\"-\"))').dlid.sort_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der xml Filterparameter (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abfagen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:\n",
    "#lineExternalNamePattern Abfrage über DLID\n",
    "xml_request_test = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + gestern + '</startDate>'\n",
    "                    '<endDate>' + gestern + '</endDate>'\n",
    "                    '<lineShortNamePattern>760</lineShortNamePattern>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abruf XML und Erstellen Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesamt VBN\n",
    "\n",
    "- Abfagen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4: und 910 aus Cloppenburg\n",
    "- lineExternalNamePattern Abfrage über DLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_request_dlid = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + gestern + '</startDate>'\n",
    "                    '<endDate>' + gestern + '</endDate>'\n",
    "                    '<lineExternalNamePattern>de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:,de:VBN-VGC:910:</lineExternalNamePattern>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "\t<soapenv:Header/>\n",
      "\t<soapenv:Body>\n",
      "\t\t<v14:createArchiveJob>\n",
      "\t\t\t<filter>\n",
      "\t\t\t\t<clientId>PMQmY5p9y8kmoTno</clientId>\n",
      "\t\t\t\t<startDate>2023-12-03</startDate>\n",
      "\t\t\t\t<endDate>2023-12-03</endDate>\n",
      "\t\t\t\t<lineExternalNamePattern>de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:</lineExternalNamePattern>\n",
      "\t\t\t\t<hasRealtime>ALL</hasRealtime>\n",
      "\t\t\t</filter>\n",
      "\t\t</v14:createArchiveJob>\n",
      "\t</soapenv:Body>\n",
      "</soapenv:Envelope>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_pretty_xml(xml_request_dlid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 08:21:51.085009 Status: IN_PROCESS\n",
      "2023-12-04 08:22:11.255744 Status: IN_PROCESS\n",
      "2023-12-04 08:22:31.450248 Status: IN_PROCESS\n",
      "2023-12-04 08:22:51.633984 Status: IN_PROCESS\n",
      "2023-12-04 08:23:11.804057 Status: IN_PROCESS\n",
      "2023-12-04 08:23:31.995925 Status: IN_PROCESS\n",
      "2023-12-04 08:23:52.202246 Status: IN_PROCESS\n",
      "2023-12-04 08:24:12.381432 Status: IN_PROCESS\n",
      "2023-12-04 08:24:32.571463 Status: IN_PROCESS\n",
      "2023-12-04 08:24:52.737320 Status: IN_PROCESS\n",
      "2023-12-04 08:25:12.931725 Status: IN_PROCESS\n",
      "2023-12-04 08:25:33.111809 Status: IN_PROCESS\n",
      "2023-12-04 08:25:53.297926 Status: IN_PROCESS\n",
      "2023-12-04 08:26:14.482230 Status: IN_PROCESS\n",
      "2023-12-04 08:26:34.673222 Status: IN_PROCESS\n",
      "2023-12-04 08:26:54.832146 Status: IN_PROCESS\n",
      "2023-12-04 08:27:14.997640 Status: IN_PROCESS\n",
      "2023-12-04 08:27:35.170465 Status: COMPLETED\n",
      "api_xml/rt_archiv_2023-12-03_vbn.xml gespeichert\n"
     ]
    }
   ],
   "source": [
    "xml_path = 'api_xml'\n",
    "xml_out = 'rt_archiv_'+gestern+'_vbn.xml'\n",
    "\n",
    "request_xml(14, xml_request_dlid, xml_path=xml_path, xml_out=xml_out)\n",
    "df_rt_vbn = import_rt_xml_to_df(os.path.join(xml_path, xml_out))\n",
    "\n",
    "xml_to_targz(xml_path=xml_path, xml_file=xml_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Für Zusatzfahrten\n",
    "\n",
    "- Abfragen aller Daten über die RTTypes\n",
    "    - REALTIME_EXTRA und weitere\n",
    "    - DEVIATION_OF_SCHEDULED\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_request_zusatz_umleitung = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + gestern + '</startDate>'\n",
    "                    '<endDate>' + gestern + '</endDate>'\n",
    "                    '<filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>REALTIME_EXTRA_REPLACEMENT</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>REALTIME_EXTRA_REPORTED</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>REALTIME_EXTRA_MAINTENANCE</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>DEVIATION_OF_REALTIME_EXTRA</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>DEVIATION_OF_REPLACEMENT</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>SUPPLEMENTARY</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>UNKNOWN</filterJourneyRtTypeList>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "\t<soapenv:Header/>\n",
      "\t<soapenv:Body>\n",
      "\t\t<v14:createArchiveJob>\n",
      "\t\t\t<filter>\n",
      "\t\t\t\t<clientId>PMQmY5p9y8kmoTno</clientId>\n",
      "\t\t\t\t<startDate>2023-12-03</startDate>\n",
      "\t\t\t\t<endDate>2023-12-03</endDate>\n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_REPLACEMENT</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_REPORTED</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA_MAINTENANCE</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_REALTIME_EXTRA</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_REPLACEMENT</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>SUPPLEMENTARY</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>UNKNOWN</filterJourneyRtTypeList>\n",
      "\t\t\t\t<hasRealtime>ALL</hasRealtime>\n",
      "\t\t\t</filter>\n",
      "\t\t</v14:createArchiveJob>\n",
      "\t</soapenv:Body>\n",
      "</soapenv:Envelope>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_pretty_xml(xml_request_zusatz_umleitung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-04 08:30:20.222377 Status: IN_PROCESS\n",
      "2023-12-04 08:30:40.390224 Status: IN_PROCESS\n",
      "2023-12-04 08:31:00.604697 Status: IN_PROCESS\n",
      "2023-12-04 08:31:20.834629 Status: IN_PROCESS\n",
      "2023-12-04 08:31:41.024925 Status: IN_PROCESS\n",
      "2023-12-04 08:32:01.251235 Status: IN_PROCESS\n",
      "2023-12-04 08:32:21.443627 Status: IN_PROCESS\n",
      "2023-12-04 08:32:41.619743 Status: IN_PROCESS\n",
      "2023-12-04 08:33:01.797297 Status: IN_PROCESS\n",
      "2023-12-04 08:33:22.978963 Status: IN_PROCESS\n",
      "2023-12-04 08:33:43.182406 Status: IN_PROCESS\n",
      "2023-12-04 08:34:03.367093 Status: IN_PROCESS\n",
      "2023-12-04 08:34:23.546239 Status: IN_PROCESS\n",
      "2023-12-04 08:34:43.728836 Status: IN_PROCESS\n",
      "2023-12-04 08:35:03.889991 Status: COMPLETED\n",
      "api_xml/rt_archiv_2023-12-03_zusatz.xml gespeichert\n"
     ]
    }
   ],
   "source": [
    "xml_path = 'api_xml'\n",
    "xml_out = 'rt_archiv_'+gestern+'_zusatz.xml'\n",
    "\n",
    "request_xml(14, xml_request_zusatz_umleitung, xml_path=xml_path, xml_out=xml_out)\n",
    "df_rt_zusatz = import_rt_xml_to_df(os.path.join(xml_path, xml_out))\n",
    "\n",
    "xml_to_targz(xml_path=xml_path, xml_file=xml_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       HOCHBAHN-IDS\n",
       "4                 ue\n",
       "7                vwg\n",
       "18               vgb\n",
       "51               vos\n",
       "59                DB\n",
       "64              BSAG\n",
       "78          regiobus\n",
       "88             bruns\n",
       "90            vrr_nx\n",
       "101          vrr_nwb\n",
       "116             DBRB\n",
       "288             bsvg\n",
       "290               me\n",
       "539                -\n",
       "608            !ADD!\n",
       "762            goevb\n",
       "889         eurobahn\n",
       "1201          NWB-LS\n",
       "1310       kvg-stade\n",
       "1376           rebus\n",
       "1560             RBB\n",
       "1562       IVU-Regio\n",
       "2182             vej\n",
       "3305         vrahden\n",
       "Name: clientid, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rt_zusatz.clientid.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "journeyrttype\n",
       "DEVIATION_OF_SCHEDULED         4688\n",
       "REALTIME_EXTRA                 1620\n",
       "REALTIME_EXTRA_REPORTED         332\n",
       "DEVIATION_OF_REALTIME_EXTRA       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rt_zusatz.journeyrttype.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xlsx = 'rt_' + gestern + '.xlsx' #lokal für Test\n",
    "xlsx = '/var/www/rt2/rt_' + gestern + '.xlsx'\n",
    "sn01 = '01 extra_deviation'\n",
    "sn02 = '02 stat_extra_deviation'\n",
    "sn03 = '03 alle_fahrten_vbn'\n",
    "sn04 = '04 ausfall vbn'\n",
    "with pd.ExcelWriter(xlsx, engine='xlsxwriter') as writer:\n",
    "    #Zusatzfahrten\n",
    "    liste_ausschluss_betreiber = ['ue', 'HOCHBAHN-IDS', 'bsvg', 'vos', 'regiobus']\n",
    "    df_rt_zusatz[~(df_rt_zusatz['clientid'].isin(liste_ausschluss_betreiber))].sort_values(['clientid', 'lineid']).to_excel(excel_writer=writer, sheet_name=sn01, index=False)\n",
    "    writer.sheets[sn01].autofilter('A1:O{}'.format(df_rt_zusatz.shape[0]))\n",
    "    writer.sheets[sn01].freeze_panes(1,0)\n",
    "    writer.sheets[sn01].set_column('E:G', 20)\n",
    "    writer.sheets[sn01].set_column('C:C', 20)\n",
    "    writer.sheets[sn01].set_column('A:A', 20)\n",
    "    writer.sheets[sn01].set_column('M:M', 25)\n",
    "    writer.sheets[sn01].set_column('O:O', 25)\n",
    "    # Statistik Abweichungen\n",
    "    df_stat_extra_dev = df_rt_zusatz[['clientid', 'journeyrttype']].value_counts().reset_index()\n",
    "    df_stat_extra_dev.to_excel(excel_writer=writer, sheet_name=sn02, index=False)\n",
    "    writer.sheets[sn02].freeze_panes(1,0)\n",
    "    writer.sheets[sn02].set_column('A:C', 20)\n",
    "    writer.sheets[sn02].autofilter('A1:O{}'.format(df_stat_extra_dev.shape[0]))\n",
    "    writer.sheets[sn02].set_column('A:B', 25)\n",
    "    #Alle Fahrten im VBN (de:VBN:*)\n",
    "    df_rt_vbn.sort_values(['vu', 'lineid', 'fnr']).to_excel(excel_writer=writer, sheet_name=sn03, index=False)\n",
    "    writer.sheets[sn03].autofilter('A1:O{}'.format(df_rt_vbn.shape[0]))\n",
    "    writer.sheets[sn03].freeze_panes(1,0)\n",
    "    writer.sheets[sn03].set_column('E:G', 20)\n",
    "    writer.sheets[sn03].set_column('M:M', 25)\n",
    "    writer.sheets[sn03].set_column('O:O', 20)\n",
    "    #Statistik Ausfälle im VBN\n",
    "    df_ausfall_vbn = df_rt_vbn[['vu', 'cancelled_kum']].groupby(by='vu', group_keys=True).agg( anzahl=pd.NamedAgg(column=\"cancelled_kum\", aggfunc=\"count\"), \\\n",
    "                                                                                          ausfall=pd.NamedAgg(column=\"cancelled_kum\", aggfunc=\"sum\")).reset_index()\n",
    "    df_ausfall_vbn['anteil_ausfall_proz'] = df_ausfall_vbn.apply(lambda x: int(x.ausfall / x.anzahl * 1000)/10, axis = 1)\n",
    "    df_ausfall_vbn.to_excel(excel_writer=writer, sheet_name=sn04, index=False)  \n",
    "    writer.sheets[sn04].set_column('A:A', 30)\n",
    "    writer.sheets[sn04].set_column('B:D', 22)\n",
    "    writer.sheets[sn04].freeze_panes(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schreiben der Tabellen nach Duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiales Anlegen der DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "    con.sql(\"CREATE or replace TABLE fahrten_\"+gestern.replace('-', '_')+\" AS SELECT * FROM df_rt_vbn\")\n",
    "    con.sql(\"CREATE or replace TABLE zusatz_\"+gestern.replace('-', '_')+\" AS SELECT * FROM df_rt_zusatz\")\n",
    "    con.sql(\"CREATE OR REPLACE TABLE lin_dm AS SELECT * FROM df_lin_dm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren / Parsen aus xml und Anfügen von Datensätzen\n",
    "https://duckdb.org/docs/api/python/data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb Cell 41\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m xml_import_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mapi_xml\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m tar_gz \u001b[39m=\u001b[39m xml_import \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.tar.gz\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(xml_import_path, tar_gz)):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mwith\u001b[39;00m tarfile\u001b[39m.\u001b[39mopen(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(xml_import_path, tar_gz), \u001b[39m'\u001b[39m\u001b[39mr:gz\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m tar:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# Extract all files to the specified directory    \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwortmann/home/zvbn/python/rt2/rt_api2_01_duckdb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         tar\u001b[39m.\u001b[39mextractall(xml_import_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "xml_import = 'rt_archiv_2023-11-21.xml'\n",
    "xml_import_path = 'api_xml'\n",
    "\n",
    "tar_gz = xml_import + '.tar.gz'\n",
    "\n",
    "if os.path.exists(os.path.join(xml_import_path, tar_gz)):\n",
    "    with tarfile.open(os.path.join(xml_import_path, tar_gz), 'r:gz') as tar:\n",
    "        # Extract all files to the specified directory    \n",
    "        tar.extractall(xml_import_path)\n",
    "else:\n",
    "    print('no tar.gz')\n",
    "\n",
    "df_rt_app = import_rt_xml_to_df(os.path.join(xml_import_path, xml_import))\n",
    "\n",
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "    con.sql(\"INSERT INTO fahrten SELECT * FROM df_rt_app\")\n",
    "    print(con.sql('select datum, count(datum) fahrten_tag from fahrten group by datum order by datum;'))\n",
    "\n",
    "\n",
    "with tarfile.open(os.path.join(xml_import_path, tar_gz), 'w:gz') as archive:\n",
    "    # Add files to the tarball\n",
    "    archive.add(os.path.join(xml_import_path, xml_import), arcname= xml_import)\n",
    "                \n",
    "os.remove(os.path.join(xml_import_path, xml_import))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    print(con.sql('select datum, count(datum) fartehn_tag from fahrten group by datum order by datum;'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with duckdb.connect('db/rt.db') as con:\n",
    "#    con.sql(\"delete from fahrten where datum >= '2023-11-11'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    print(con.sql('show tables;'))\n",
    "    print(con.sql('describe fahrten;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "    df_false_rt = con.sql(\"SELECT * FROM fahrten WHERE hasRealtime = False ORDER BY lineid\").df()\n",
    "    df_rt = con.sql(\"SELECT * FROM fahrten\").df()\n",
    "    df_fahrten_buendel = con.sql(\"SELECT * FROM fahrten LEFT JOIN lin_dm ON fahrten.lineid_short = lin_dm.dlid ORDER BY lineid\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = df_rt.lineid_short.drop_duplicates().sort_values()\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ermitteln der ausgefallenen Fahrten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt.query('reported_cancelled == True and journey_cancelled == \"False\" and datum == \"2023-11-21\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[['datum', 'cancelled_kum', 'fnr']].groupby('datum').agg({'fnr': 'count', 'cancelled_kum':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[['datum', 'vu','cancelled_kum', 'fnr']].groupby(['datum', 'vu']).agg({'fnr': 'count', 'cancelled_kum':'sum'}).reset_index().to_excel('reports/ausfall.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[df_rt.cancelled_kum == True][['datum','vu' ,'cancelled_kum', 'fnr']].groupby(['datum', 'vu']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der Werte im Fahrtverlauf (noch aus der alten Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "format = '%Y-%m-%dT%H:%M:%S'\n",
    "log_verlauf_arr = []\n",
    "\n",
    "root = ET.fromstring(r.text)\n",
    "for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "    for journey in child.iter('journey'):\n",
    "        rt = rt_func.isnone(journey.find('hasRealtime'))\n",
    "        deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "        fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "        lineshortname = str(rt_func.isnone(journey.find('lineShortName'))).strip()\n",
    "        ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "        journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "        operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format).strftime('%Y-%m-%d')\n",
    "        ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "        reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "\n",
    "    for details in child.iter('details'):\n",
    "        index = rt_func.isnone(details.find('index'))\n",
    "        for ddelay in details.iter('departureDelay'):\n",
    "            dep_del = rt_func.isnone_delay(ddelay.find('delay'))\n",
    "\n",
    "        for adelay in details.iter('arrivalDelay'):\n",
    "            arr_del = rt_func.isnone_delay(adelay.find('delay'))\n",
    "        \n",
    "        canc = rt_func.isnone(details.find('cancelled'))\n",
    "        \n",
    "        additional =  rt_func.isnone(details.find('additional'))\n",
    "\n",
    "        for station in details.iter('station'):\n",
    "            lat = int(station.find('latitude').text)/1000000\n",
    "            lon = int(station.find('longitude').text)/1000000\n",
    "            snr = station.find('stationExternalNumber').text\n",
    "            if station.find('stationName') is not None:\n",
    "                sname = station.find('stationName').text\n",
    "            else:\n",
    "                sname = '-'\n",
    "        \n",
    "        for dschedule in details.iter('scheduleDepartureTime'):\n",
    "            dschedtime= dschedule.find('scheduleTime')\n",
    "            if dschedtime is not None:\n",
    "                dschedtime = dt.datetime.strptime(dschedtime.text[:-6], format).strftime('%Y%m%d%H%M%S') #Umwandlung der Zeitformat da in 3.6 kein ISO-Format vorhanden\n",
    "            else:\n",
    "                dschedtime =''\n",
    "        for aschedule in details.iter('scheduleArrivalTime'):\n",
    "            aschedtime = aschedule.find('scheduleTime')\n",
    "            if aschedtime is not None:\n",
    "                aschedtime = dt.datetime.strptime(aschedtime.text[:-6], format).strftime('%Y%m%d%H%M%S')\n",
    "            else: \n",
    "                aschedtime =''\n",
    "        \n",
    "        # Schreiben in die Datei log_verlauf\n",
    "        #print(operday, journeyOperator, deviceid, lineshortname, ex_lineid, fnr, index, rt, dschedtime, aschedtime, \\\n",
    "        #      dep_del, arr_del, snr, sname, lat, lon, canc, additional, reported_cancelled,ts_reported_cancelled,file=log_verlauf, sep=';')\n",
    "        log_verlauf_arr.append([operday, journeyOperator, deviceid, lineshortname, ex_lineid, \n",
    "                                fnr, index, rt, dschedtime, aschedtime, dep_del, arr_del, snr, sname, lat, lon, canc, additional, ts_reported_cancelled, reported_cancelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort = pd.DataFrame(log_verlauf_arr, columns= ['tag', 'journeyOperator', 'deviceid', 'linie', 'externallinid', 'fahrtnummer', 'index', 'realtime', 'ab_soll', \n",
    "                                  'an_soll', 'ab_delay', 'an_delay', 'stationnumber', 'stationname', 'lat', 'lon', 'cancelled', 'additional', 'ts_reported_cancelled', 'reported_cancelled'])\n",
    "\n",
    "log_sort['index'] = log_sort['index'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rausfiltern der Fahrten ohne Fahrtnummer (aufgetreten 9/2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort = log_sort[~(log_sort.fahrtnummer == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort['fahrtnummer'] = log_sort['fahrtnummer'].astype(np.int32)\n",
    "log_sort['stationnumber'] = log_sort['stationnumber'].astype(np.int32)\n",
    "log_sort['linie'] = log_sort['linie'].astype(str)\n",
    "log_sort['stationname'] = log_sort['stationname'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export als Parquet\n",
    "#log_sort.astype({'linie':'string'}).to_parquet('log/verlauf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort['ab_soll'] = log_sort.apply(lambda x: rt_func.format_zeit(x['ab_soll']), axis=1)\n",
    "log_sort['an_soll'] = log_sort.apply(lambda x: rt_func.format_zeit(x['an_soll']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe der Werte je Fahrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrt_array = []\n",
    "root = ET.fromstring(r.text)\n",
    "for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "    for journey in child.iter('journey'):\n",
    "        rt = rt_func.isnone(journey.find('hasRealtime'))\n",
    "        deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "        fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "        lineshortname = rt_func.isnone(journey.find('lineShortName'))\n",
    "        ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "        dlid_pre = ex_lineid.split(':')[0:3]\n",
    "        dlid = ':'.join(dlid_pre)\n",
    "        dest = rt_func.isnone(journey.find('destination'))\n",
    "        operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format).strftime('%Y-%m-%d')\n",
    "        op = rt_func.isnone(journey.find('journeyOperator'))\n",
    "        cancelled = rt_func.isnone(journey.find('journeyCancelled'))\n",
    "        lastupdate = rt_func.isnone(journey.find('lastJourneyDetailsUpdateTimestamp'))\n",
    "        vehicletypeschedule = rt_func.isnone(journey.find('vehicleTypeSchedule'))\n",
    "        vehicletyperealtime = rt_func.isnone(journey.find('vehicleTypeRealtime'))\n",
    "        ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "        reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "        \n",
    "        journeyRtType = rt_func.isnone(journey.find('journeyRtType'))\n",
    "        try:\n",
    "            lastupdate = dt.datetime.fromtimestamp(int(lastupdate)/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            continue\n",
    "        distance = rt_func.isnone(journey.find('distanceSchedule'))\n",
    "        \n",
    "        if journey.find('maximumDelay'):\n",
    "            #print(fnr, day)\n",
    "            for maxdelay in journey.iter('maximumDelay'):\n",
    "                #print(maxdelay.find('delay'))\n",
    "                maxd = rt_func.isnone(maxdelay.find('delay'))\n",
    "        else:\n",
    "            maxd=''\n",
    "        \n",
    "        if journey.find('departureDelay'):\n",
    "            #print(fnr, day)\n",
    "            for depdelay in journey.iter('departureDelay'):\n",
    "                ddel = rt_func.isnone(depdelay.find('delay'))\n",
    "        else:\n",
    "            ddel=''\n",
    "        \n",
    "        if journey.find('arrivalDelay'):\n",
    "            #print(fnr, day)\n",
    "            for arrdelay in journey.iter('arrivalDelay'):\n",
    "                adel = rt_func.isnone(arrdelay.find('delay'))\n",
    "        else:\n",
    "            adel=''\n",
    "        \n",
    "        if journey.find('minimumDelay'):\n",
    "            #print(fnr, day)\n",
    "            for mindelay in journey.iter('minimumDelay'):\n",
    "                mind = rt_func.isnone(mindelay.find('delay'))\n",
    "        else:\n",
    "            mind=''\n",
    "        \n",
    "        if journey.find('realArrivalTime'):\n",
    "            #print(fnr, day)\n",
    "            for realarrival in journey.iter('realArrivalTime'):\n",
    "                rat = dt.datetime.strptime(rt_func.isnone(realarrival.find('realTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            rat=''        \n",
    "        \n",
    "        if journey.find('realDepartureTime'):\n",
    "            for realdeparture in journey.iter('realDepartureTime'):\n",
    "                rdt = dt.datetime.strptime(rt_func.isnone(realdeparture.find('realTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            rdt=''\n",
    "\n",
    "        if journey.find('scheduleDepartureTime'):\n",
    "            for realdeparture in journey.iter('scheduleDepartureTime'):\n",
    "                sdt = dt.datetime.strptime(rt_func.isnone(realdeparture.find('scheduleTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            sdt=''\n",
    "            \n",
    "        if journey.find('scheduleArrivalTime'):\n",
    "            for realdeparture in journey.iter('scheduleArrivalTime'):\n",
    "                sat = dt.datetime.strptime(rt_func.isnone(realdeparture.find('scheduleTime'))[:-6], format)\\\n",
    "                .strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            sat=''       \n",
    "        \n",
    "        if journey.find('scheduleDepartureStation'):\n",
    "            for dep_station in journey.iter('scheduleDepartureStation'):\n",
    "                sdst = rt_func.isnone(dep_station.find('stationName'))               \n",
    "        else:\n",
    "            sdst=''\n",
    "        \n",
    "        if journey.find('scheduleArrivalStation'):\n",
    "            for arr_station in journey.iter('scheduleArrivalStation'):\n",
    "                sast = rt_func.isnone(arr_station.find('stationName'))               \n",
    "        else:\n",
    "            sast=''\n",
    "        fahrt_array.append([lineshortname, ex_lineid, operday, fnr, rt, op, dest, mind, maxd,  cancelled, lastupdate, \\\n",
    "              distance, sdt, sat, rdt, rat, dlid,sdst, sast,ddel, adel, deviceid, journeyRtType, vehicletypeschedule, vehicletyperealtime, \\\n",
    "                reported_cancelled, ts_reported_cancelled])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.11.0 ('geo_311')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dcfa513e1bebd633c31802f439dbc67afc7b6eca02dfa4380fc98826b2b9354"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
