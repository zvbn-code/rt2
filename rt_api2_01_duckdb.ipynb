{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abfrage Schnittstelle und Ablage in DuckDBzum Hafas Echtzeit-Archiv Produktiv / Demo-System / Ablage in DuckDB\n",
    "\n",
    "Stand: 18.11.2023\n",
    "\n",
    "#### Aufgaben\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "import datetime as dt\n",
    "import time\n",
    "import calendar\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "import duckdb\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from sqlalchemy import create_engine #als Alternative zu Mysql pyscopg2 Connector\n",
    "from sqlalchemy import text\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rt_archiv_func_07' from '/home/zvbn/python/rt2/rt_archiv_func_07.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import para\n",
    "import rt_archiv_func_07 as rt_func #Import der benutzerdefinierten Funktionen\n",
    "reload(rt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen der Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufrufen der Abfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_xml(api_version, xml_request, out_name):\n",
    "    #Zugriff auf Hafas RT Archiv Produktiv System und Zugriffsschlüssel\n",
    "    myUrl = 'https://fahrplaner.vbn.de/archive/services/archiveExportService/v'+str(api_version)+'?wsdl'\n",
    "    clientID = 'PMQmY5p9y8kmoTno'\n",
    "    req_ini = requests.post(myUrl, data=xml_request)\n",
    "    root = ET.fromstring(req_ini.text)\n",
    "    #Ermitteln der Export ID\n",
    "    for child in root.iter('exportId'):\n",
    "        #print(child.tag, child.attrib, child.text)\n",
    "        exportId = child.text\n",
    "    xml_status = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/>'\n",
    "                    '<soapenv:Body>'\n",
    "                        '<v'+str(api_version)+':getArchiveExportStatus>'\n",
    "                            '<exportId>' + exportId + '</exportId>'\n",
    "                        '</v'+str(api_version)+':getArchiveExportStatus>'\n",
    "                    '</soapenv:Body>'\n",
    "              '</soapenv:Envelope>')\n",
    "    #Abfragen und Warten auf Completed\n",
    "    status = ''\n",
    "    time.sleep(2) # initiales Warten auf Beendigung\n",
    "    while status != 'COMPLETED':\n",
    "        r = requests.post(myUrl, data=xml_status)\n",
    "        #print(r, '\\n',r.text)\n",
    "        root = ET.fromstring(r.text)\n",
    "        for child in root.iter('status'):\n",
    "            #print(child.tag, child.attrib, child.text)\n",
    "            status = child.text\n",
    "            print(f'{dt.datetime.now()} Status: {status}')\n",
    "            if status != 'COMPLETED': # Pause falls Job nicht beendet (Status nicht completed d.h. in process)\n",
    "                time.sleep(20) # Pause von 20 Sekunden bis zur nächsten Abfrage des Status\n",
    "    \n",
    "    # Afrage nach Beendigung\n",
    "    xml_jl = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "                 '<soapenv:Header/><soapenv:Body>'\n",
    "                    '<v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                       '<exportId>' + exportId + '</exportId>'              \n",
    "                     '</v'+str(api_version)+':getArchiveJourneyList>'\n",
    "                 '</soapenv:Body>'\n",
    "          '</soapenv:Envelope>')\n",
    "    \n",
    "    r = requests.post(myUrl, data=xml_jl)\n",
    "\n",
    "    #Ausgabe des Ergebnis XML\n",
    "    \n",
    "    xml_path = 'api_xml'\n",
    "    xml_out = 'rt_archiv_' + out_name + '.xml'\n",
    "    \n",
    "    dom = xml.dom.minidom.parseString(r.text)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    \n",
    "    jl = open(os.path.join(xml_path, xml_out), 'w')\n",
    "    print(pretty_xml_as_string, file = jl)\n",
    "    print(os.path.join(xml_path, xml_out), 'gespeichert')\n",
    "\n",
    "    jl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des xml und Umwandeln nach Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_rt_xml_to_df(xml):\n",
    "    format_date = '%Y-%m-%dT%H:%M:%S'\n",
    "    lop = []\n",
    "    \n",
    "    # create element tree object \n",
    "    tree = ET.parse(xml)\n",
    "    \n",
    "    # get root element \n",
    "    root = tree.getroot() \n",
    "\n",
    "    for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "        for journey in child.iter('journey'):\n",
    "            operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format_date).strftime('%Y-%m-%d')\n",
    "            fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "\n",
    "            deviceId = rt_func.isnone(journey.find('deviceId'))\n",
    "            clientId = rt_func.split_deviceid(journey.find('deviceId'))            \n",
    "\n",
    "            journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "            ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "            ex_linid_short = ':'.join(ex_lineid.split(':')[0:3])\n",
    "            lineshortname = rt_func.isnone(journey.find('lineShortName'))\n",
    "            destination = rt_func.isnone(journey.find('destination'))\n",
    "\n",
    "            hasRealtime = rt_func.isnone_boolean(journey.find('hasRealtime'))\n",
    "            journeyRtType = rt_func.isnone(journey.find('journeyRtType'))            \n",
    "\n",
    "            journeycancelled = rt_func.isnone(journey.find('journeyCancelled')).capitalize()\n",
    "            ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "            reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "            cancelled_kum = True if str(reported_cancelled) == 'True' else True if str(journeycancelled) == 'True' else False\n",
    "            \n",
    "            lop.append([operday, fnr, destination, hasRealtime, journeyOperator, ex_lineid, ex_linid_short, lineshortname, \\\n",
    "                        reported_cancelled, journeycancelled, ts_reported_cancelled, cancelled_kum, deviceId, clientId, journeyRtType])\n",
    "            \n",
    "            child.clear()\n",
    "\n",
    "    df_fahrten = pd.DataFrame(lop, columns=['datum','fnr' ,'destination','hasRealtime' ,'vu', 'lineid', 'lineid_short', 'lineshort', \\\n",
    "                                            'reported_cancelled', 'journey_cancelled','ts_reported_cancelled' ,'cancelled_kum', 'deviceid', 'clientid', 'journeyrttype' ])\n",
    "    return df_fahrten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausgabe als formatiertes xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testen des XML mit schöner Ausgabe\n",
    "def print_pretty_xml(xml_request):\n",
    "    dom = xml.dom.minidom.parseString(xml_request)\n",
    "    pretty_xml_as_string = dom.toprettyxml()\n",
    "    print(pretty_xml_as_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ermitteln verschiedener Zeitpunkte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heute: 20231124 \n",
      "Jetzt: 202311241126 \n",
      "Gestern: 2023-11-23 \n",
      "Vorgestern: 2023-11-22 \n",
      "Vor vier Wochen: 2023-10-27 \n",
      "Vor sechs Wochen: 2023-10-13\n",
      "vor einer Woche 2023-11-17\n",
      "Erster Tag des letzen Monats: 2023-10-01 und letzter Tag:  2023-10-31\n"
     ]
    }
   ],
   "source": [
    "#Ermitteln der Zeitstempel\n",
    "jetzt = dt.datetime.now().strftime('%Y%m%d%H%M')\n",
    "heute = dt.date.today().strftime('%Y%m%d')\n",
    "heute_ll = dt.datetime.now().strftime('%d.%m.%Y %H:%M')\n",
    "gestern = (dt.date.today() - timedelta(1)).strftime('%Y-%m-%d')\n",
    "vorgestern = (dt.datetime.now() - timedelta(2)).strftime('%Y-%m-%d')\n",
    "vorvierwochen = (dt.date.today() - timedelta(28)).strftime('%Y-%m-%d')\n",
    "vorsechswochen = (dt.date.today() - timedelta(42)).strftime('%Y-%m-%d')\n",
    "letztesiebentage = (dt.date.today() - timedelta(7)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f'Heute: {heute} \\nJetzt: {jetzt} \\nGestern: {gestern} \\nVorgestern: {vorgestern} \\nVor vier Wochen: {vorvierwochen} \\nVor sechs Wochen: {vorsechswochen}')\n",
    "print('vor einer Woche ' + letztesiebentage)\n",
    "#ermitteln des letzten Monats\n",
    "\n",
    "today = dt.date.today()\n",
    "first = today.replace(day=1) #auf den ersten des aktuellen Monats setzen\n",
    "lastmonth_last = first - dt.timedelta(days=1) # 1 Tag abziehen vom ersten Tag \n",
    "lastmonth_first = lastmonth_last.replace(day=1) #ersetzen des ersten Tages\n",
    "lastmonth_first_str = str(lastmonth_first)\n",
    "lastmonth_last_str = str(lastmonth_last)\n",
    "print(f'Erster Tag des letzen Monats: {lastmonth_first} und letzter Tag:  {lastmonth_last}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aktuelle Version der Schnittstellenbeschreibung unter \n",
    "- docs/SmartVMS Export API v12.pdf\n",
    "- docs/SmartVMS Export API v14_datatypes-1.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = 14 #14 auf demo und ab August 2023 auf prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zugriff auf Hafas RT Archiv Produktiv System und Zugriffsschlüssel\n",
    "myUrl = 'https://fahrplaner.vbn.de/archive/services/archiveExportService/v'+str(api_version)+'?wsdl'\n",
    "clientID = 'PMQmY5p9y8kmoTno'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einlesen der Linienliste / Zuordnung Bündel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einlesen aus der lokalen DM Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbindung erfolgreich -lokale Datei aktualisiert\n"
     ]
    }
   ],
   "source": [
    "#Zugriff auf die lokale Datenbank auf dem Wortmann Debian Server\n",
    "\n",
    "try:\n",
    "    engine = create_engine(\"postgresql+psycopg2://postgres:\"+para.key_dm_db+\"@127.0.0.1:5432/zvbn_postgis\")\n",
    "    #conn_dm = psycopg2.connect(database='zvbn_postgis', user='postgres', password=para.key_dm_db, host = '127.0.0.1')\n",
    "    sql_lin = 'SELECT nummer AS linie, buendel, \\'\\' AS rt_operator, ebene, dlid, id \\\n",
    "        FROM basis.linien \\\n",
    "        WHERE buendel IS NOT NULL AND aktiv IS TRUE \\\n",
    "        ORDER BY buendel, ebene, nummer'\n",
    "    sql_buendel = 'SELECT * FROM basis.lin_buendel'\n",
    "    df_lin_dm =  pd.read_sql(text(sql_lin), engine.connect())\n",
    "    df_buendel = pd.read_sql(text(sql_buendel), engine.connect())\n",
    "    df_lin_dm.to_csv('input/linien_dm.csv', sep=';', index=False)\n",
    "    print('Verbindung erfolgreich -lokale Datei aktualisiert')\n",
    "except:\n",
    "    df_lin_dm = pd.read_csv('input/linien_dm.csv', sep=';') #aktuelle Zuordnung Linie zu Bündel aus DM\n",
    "    print(f'Verbindung nicht erfolgreich - Verwendung lokale Datei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#External Linids, die nicht mit de:VBN starten\n",
    "#df_lin_dm.fillna('-').query('not (dlid.str.startswith(\"de:VBN\") or dlid.str.startswith(\"-\"))').dlid.sort_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erstellen der Filterparameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papermill Parameter\n",
    "\n",
    "- Zelle ist mit Tags 'Parameters' versehen \n",
    "Parameter - nicht ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#auswahl =  ['DEL','OHZ West', 'OHZ Ost', 'OHZ Mitte', 'VER Nord'] #kann bei python Skript auch über Parameter Command Promppt gesteuert werden int(sys.argv[1]) oder Papermill\n",
    "auswahl = ['AM West']\n",
    "startDate = '2023-11-22'\n",
    "#endDate   = gestern\n",
    "#startDate = letztesiebentage #Beginn des Auswertungszeitraums\n",
    "endDate   = '2023-11-22'\n",
    "startendDate = '2023-11-22'\n",
    "\n",
    "#endDate  = gestern #Ende der Auswertung\n",
    "art = \"regional\" #Einstellen stadt oder regional für die Erstellung\n",
    "suffix = \"\" #besondere Endung z.B. für den Monatsbericht\n",
    "output_xml = False #Ob xml als Text ausgegeben werden soll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abfagen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:\n",
    "#lineExternalNamePattern Abfrage über DLID\n",
    "xml_request_test = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + startDate + '</startDate>'\n",
    "                    '<endDate>' + endDate + '</endDate>'\n",
    "                    '<lineShortNamePattern>760</lineShortNamePattern>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufrufen der Funktion Abruf XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesamt VBN\n",
    "\n",
    "- Abfagen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:\n",
    "- lineExternalNamePattern Abfrage über DLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_request_dlid = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + gestern + '</startDate>'\n",
    "                    '<endDate>' + gestern + '</endDate>'\n",
    "                    '<lineExternalNamePattern>de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:</lineExternalNamePattern>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "\t<soapenv:Header/>\n",
      "\t<soapenv:Body>\n",
      "\t\t<v14:createArchiveJob>\n",
      "\t\t\t<filter>\n",
      "\t\t\t\t<clientId>PMQmY5p9y8kmoTno</clientId>\n",
      "\t\t\t\t<startDate>2023-11-23</startDate>\n",
      "\t\t\t\t<endDate>2023-11-23</endDate>\n",
      "\t\t\t\t<lineExternalNamePattern>de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:</lineExternalNamePattern>\n",
      "\t\t\t\t<hasRealtime>ALL</hasRealtime>\n",
      "\t\t\t</filter>\n",
      "\t\t</v14:createArchiveJob>\n",
      "\t</soapenv:Body>\n",
      "</soapenv:Envelope>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_pretty_xml(xml_request_dlid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-24 11:27:21.268113 Status: NEW\n",
      "2023-11-24 11:27:41.439248 Status: IN_PROCESS\n",
      "2023-11-24 11:28:01.625315 Status: IN_PROCESS\n",
      "2023-11-24 11:28:21.796751 Status: IN_PROCESS\n",
      "2023-11-24 11:28:41.980491 Status: IN_PROCESS\n",
      "2023-11-24 11:29:02.178849 Status: IN_PROCESS\n",
      "2023-11-24 11:29:22.367566 Status: IN_PROCESS\n",
      "2023-11-24 11:29:42.557746 Status: IN_PROCESS\n",
      "2023-11-24 11:30:02.732795 Status: IN_PROCESS\n",
      "2023-11-24 11:30:22.906739 Status: IN_PROCESS\n",
      "2023-11-24 11:30:43.071903 Status: IN_PROCESS\n",
      "2023-11-24 11:31:03.297594 Status: IN_PROCESS\n",
      "2023-11-24 11:31:23.464685 Status: IN_PROCESS\n",
      "2023-11-24 11:31:43.658346 Status: IN_PROCESS\n",
      "2023-11-24 11:32:03.833367 Status: IN_PROCESS\n",
      "2023-11-24 11:32:23.994112 Status: IN_PROCESS\n",
      "2023-11-24 11:32:44.167299 Status: IN_PROCESS\n",
      "2023-11-24 11:33:04.340356 Status: IN_PROCESS\n",
      "2023-11-24 11:33:24.512978 Status: IN_PROCESS\n",
      "2023-11-24 11:33:44.678316 Status: IN_PROCESS\n",
      "2023-11-24 11:34:04.871120 Status: IN_PROCESS\n",
      "2023-11-24 11:34:25.045258 Status: IN_PROCESS\n",
      "2023-11-24 11:34:45.222790 Status: IN_PROCESS\n",
      "2023-11-24 11:35:05.389216 Status: IN_PROCESS\n",
      "2023-11-24 11:35:25.566433 Status: IN_PROCESS\n",
      "2023-11-24 11:35:45.752674 Status: IN_PROCESS\n",
      "2023-11-24 11:36:05.935461 Status: IN_PROCESS\n",
      "2023-11-24 11:36:26.129133 Status: IN_PROCESS\n",
      "2023-11-24 11:36:46.304406 Status: IN_PROCESS\n",
      "2023-11-24 11:37:06.463352 Status: IN_PROCESS\n",
      "2023-11-24 11:37:26.629745 Status: IN_PROCESS\n",
      "2023-11-24 11:37:46.800026 Status: IN_PROCESS\n",
      "2023-11-24 11:38:07.011768 Status: IN_PROCESS\n",
      "2023-11-24 11:38:27.184493 Status: IN_PROCESS\n",
      "2023-11-24 11:38:47.380755 Status: IN_PROCESS\n",
      "2023-11-24 11:39:07.568519 Status: IN_PROCESS\n",
      "2023-11-24 11:39:27.755783 Status: IN_PROCESS\n",
      "2023-11-24 11:39:47.955298 Status: IN_PROCESS\n",
      "2023-11-24 11:40:08.138841 Status: IN_PROCESS\n",
      "2023-11-24 11:40:28.315863 Status: IN_PROCESS\n",
      "2023-11-24 11:40:48.475732 Status: IN_PROCESS\n",
      "2023-11-24 11:41:08.677773 Status: IN_PROCESS\n",
      "2023-11-24 11:41:28.858379 Status: COMPLETED\n",
      "api_xml/rt_archiv_2023-11-23_vbn.xml gespeichert\n"
     ]
    }
   ],
   "source": [
    "request_xml(14, xml_request_dlid, gestern + '_vbn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Für Zusatzfahrten\n",
    "\n",
    "- Abfragen aller Daten für einen Tag über die Externallinid (de:VBN:* und Metronomlinien mit de:hvv:) de:VBN:*,de:hvv:RB33:,de:hvv:RB41:,de:hvv:RE4:\n",
    "- lineExternalNamePattern Abfrage über DLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml_request_zusatz_umleitung = ('<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" '\n",
    "               'xmlns:v'+str(api_version)+'=\"http://v'+str(api_version)+'.export.service.data.archive.itcs.hafas.hacon.de/\">'\n",
    "               '<soapenv:Header/><soapenv:Body><v'+str(api_version)+':createArchiveJob>'\n",
    "               '<filter>'\n",
    "                    '<clientId>' + clientID + '</clientId>'                    \n",
    "                    '<startDate>' + gestern + '</startDate>'\n",
    "                    '<endDate>' + gestern + '</endDate>'\n",
    "                    '<filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>'                \n",
    "                    '<filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>'                \n",
    "                    '<hasRealtime>ALL</hasRealtime>'\n",
    "               '</filter>'\n",
    "               '</v' + str(api_version) + ':createArchiveJob></soapenv:Body></soapenv:Envelope>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:v14=\"http://v14.export.service.data.archive.itcs.hafas.hacon.de/\">\n",
      "\t<soapenv:Header/>\n",
      "\t<soapenv:Body>\n",
      "\t\t<v14:createArchiveJob>\n",
      "\t\t\t<filter>\n",
      "\t\t\t\t<clientId>PMQmY5p9y8kmoTno</clientId>\n",
      "\t\t\t\t<startDate>2023-11-23</startDate>\n",
      "\t\t\t\t<endDate>2023-11-23</endDate>\n",
      "\t\t\t\t<filterJourneyRtTypeList>REALTIME_EXTRA</filterJourneyRtTypeList>\n",
      "\t\t\t\t<filterJourneyRtTypeList>DEVIATION_OF_SCHEDULED</filterJourneyRtTypeList>\n",
      "\t\t\t\t<hasRealtime>ALL</hasRealtime>\n",
      "\t\t\t</filter>\n",
      "\t\t</v14:createArchiveJob>\n",
      "\t</soapenv:Body>\n",
      "</soapenv:Envelope>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_pretty_xml(xml_request_zusatz_umleitung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-24 11:44:29.830617 Status: NEW\n",
      "2023-11-24 11:44:50.000809 Status: IN_PROCESS\n",
      "2023-11-24 11:45:10.192014 Status: IN_PROCESS\n",
      "2023-11-24 11:45:30.374699 Status: IN_PROCESS\n",
      "2023-11-24 11:45:50.544124 Status: IN_PROCESS\n",
      "2023-11-24 11:46:10.721829 Status: IN_PROCESS\n",
      "2023-11-24 11:46:30.906269 Status: IN_PROCESS\n",
      "2023-11-24 11:46:51.077101 Status: IN_PROCESS\n",
      "2023-11-24 11:47:11.244515 Status: IN_PROCESS\n",
      "2023-11-24 11:47:31.461171 Status: IN_PROCESS\n",
      "2023-11-24 11:47:51.645978 Status: IN_PROCESS\n",
      "2023-11-24 11:48:11.865522 Status: IN_PROCESS\n",
      "2023-11-24 11:48:32.091208 Status: IN_PROCESS\n",
      "2023-11-24 11:48:52.269259 Status: IN_PROCESS\n",
      "2023-11-24 11:49:12.456696 Status: IN_PROCESS\n",
      "2023-11-24 11:49:32.642280 Status: IN_PROCESS\n",
      "2023-11-24 11:49:52.825036 Status: IN_PROCESS\n",
      "2023-11-24 11:50:13.011005 Status: IN_PROCESS\n",
      "2023-11-24 11:50:33.199134 Status: IN_PROCESS\n",
      "2023-11-24 11:50:53.385012 Status: IN_PROCESS\n",
      "2023-11-24 11:51:13.566493 Status: IN_PROCESS\n",
      "2023-11-24 11:51:33.777361 Status: IN_PROCESS\n",
      "2023-11-24 11:51:54.027697 Status: IN_PROCESS\n",
      "2023-11-24 11:52:14.214551 Status: IN_PROCESS\n",
      "2023-11-24 11:52:34.394338 Status: IN_PROCESS\n",
      "2023-11-24 11:52:54.580893 Status: IN_PROCESS\n",
      "2023-11-24 11:53:14.772066 Status: IN_PROCESS\n",
      "2023-11-24 11:53:34.967291 Status: IN_PROCESS\n",
      "2023-11-24 11:53:55.147195 Status: COMPLETED\n",
      "api_xml/rt_archiv_2023-11-23_zusatz.xml gespeichert\n"
     ]
    }
   ],
   "source": [
    "request_xml(14, xml_request_zusatz_umleitung, gestern + '_zusatz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schreiben der Tabellen nach Duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiales Anlegen der DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "#    con.sql(\"CREATE or replace TABLE fahrten AS SELECT * FROM df_fahrten\")\n",
    "#    con.sql(\"CREATE OR REPLACE TABLE lin_dm AS SELECT * FROM df_lin_dm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importieren / Parsen aus xml und Anfügen von Datensätzen\n",
    "https://duckdb.org/docs/api/python/data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "xml_import = 'rt_archiv_2023-11-22.xml'\n",
    "xml_import_path = 'api_xml'\n",
    "df_rt_test = import_rt_xml_to_df(os.path.join(xml_import_path, xml_import))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt_test.to_excel('zusatz_fahrten.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt_test[['clientid', 'journeyrttype']].value_counts().to_excel('count_zusatz.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_import = 'rt_archiv_2023-11-21.xml'\n",
    "xml_import_path = 'api_xml'\n",
    "\n",
    "tar_gz = xml_import + '.tar.gz'\n",
    "\n",
    "if os.path.exists(os.path.join(xml_import_path, tar_gz)):\n",
    "    with tarfile.open(os.path.join(xml_import_path, tar_gz), 'r:gz') as tar:\n",
    "        # Extract all files to the specified directory    \n",
    "        tar.extractall(xml_import_path)\n",
    "else:\n",
    "    print('no tar.gz')\n",
    "\n",
    "df_rt_app = import_rt_xml_to_df(os.path.join(xml_import_path, xml_import))\n",
    "\n",
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "    con.sql(\"INSERT INTO fahrten SELECT * FROM df_rt_app\")\n",
    "    print(con.sql('select datum, count(datum) fartehn_tag from fahrten group by datum order by datum;'))\n",
    "\n",
    "\n",
    "with tarfile.open(os.path.join(xml_import_path, tar_gz), 'w:gz') as archive:\n",
    "    # Add files to the tarball\n",
    "    archive.add(os.path.join(xml_import_path, xml_import), arcname= xml_import)\n",
    "                \n",
    "os.remove(os.path.join(xml_import_path, xml_import))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    print(con.sql('select datum, count(datum) fartehn_tag from fahrten group by datum order by datum;'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with duckdb.connect('db/rt.db') as con:\n",
    "#    con.sql(\"delete from fahrten where datum >= '2023-11-11'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    print(con.sql('show tables;'))\n",
    "    print(con.sql('describe fahrten;'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with duckdb.connect('db/rt.db') as con:\n",
    "    con.install_extension(\"spatial\")\n",
    "    con.load_extension(\"spatial\")\n",
    "    df_false_rt = con.sql(\"SELECT * FROM fahrten WHERE hasRealtime = False ORDER BY lineid\").df()\n",
    "    df_rt = con.sql(\"SELECT * FROM fahrten\").df()\n",
    "    df_fahrten_buendel = con.sql(\"SELECT * FROM fahrten LEFT JOIN lin_dm ON fahrten.lineid_short = lin_dm.dlid ORDER BY lineid\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = df_rt.lineid_short.drop_duplicates().sort_values()\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ermitteln der ausgefallenen Fahrten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt.query('reported_cancelled == True and journey_cancelled == \"False\" and datum == \"2023-11-21\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[['datum', 'cancelled_kum', 'fnr']].groupby('datum').agg({'fnr': 'count', 'cancelled_kum':'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[['datum', 'vu','cancelled_kum', 'fnr']].groupby(['datum', 'vu']).agg({'fnr': 'count', 'cancelled_kum':'sum'}).reset_index().to_excel('reports/ausfall.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rt[df_rt.cancelled_kum == True][['datum','vu' ,'cancelled_kum', 'fnr']].groupby(['datum', 'vu']).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabe der Werte im Fahrtverlauf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "format = '%Y-%m-%dT%H:%M:%S'\n",
    "log_verlauf_arr = []\n",
    "\n",
    "root = ET.fromstring(r.text)\n",
    "for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "    for journey in child.iter('journey'):\n",
    "        rt = rt_func.isnone(journey.find('hasRealtime'))\n",
    "        deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "        fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "        lineshortname = str(rt_func.isnone(journey.find('lineShortName'))).strip()\n",
    "        ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "        journeyOperator = rt_func.isnone(journey.find('journeyOperator'))\n",
    "        operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format).strftime('%Y-%m-%d')\n",
    "        ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "        reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "\n",
    "    for details in child.iter('details'):\n",
    "        index = rt_func.isnone(details.find('index'))\n",
    "        for ddelay in details.iter('departureDelay'):\n",
    "            dep_del = rt_func.isnone_delay(ddelay.find('delay'))\n",
    "\n",
    "        for adelay in details.iter('arrivalDelay'):\n",
    "            arr_del = rt_func.isnone_delay(adelay.find('delay'))\n",
    "        \n",
    "        canc = rt_func.isnone(details.find('cancelled'))\n",
    "        \n",
    "        additional =  rt_func.isnone(details.find('additional'))\n",
    "\n",
    "        for station in details.iter('station'):\n",
    "            lat = int(station.find('latitude').text)/1000000\n",
    "            lon = int(station.find('longitude').text)/1000000\n",
    "            snr = station.find('stationExternalNumber').text\n",
    "            if station.find('stationName') is not None:\n",
    "                sname = station.find('stationName').text\n",
    "            else:\n",
    "                sname = '-'\n",
    "        \n",
    "        for dschedule in details.iter('scheduleDepartureTime'):\n",
    "            dschedtime= dschedule.find('scheduleTime')\n",
    "            if dschedtime is not None:\n",
    "                dschedtime = dt.datetime.strptime(dschedtime.text[:-6], format).strftime('%Y%m%d%H%M%S') #Umwandlung der Zeitformat da in 3.6 kein ISO-Format vorhanden\n",
    "            else:\n",
    "                dschedtime =''\n",
    "        for aschedule in details.iter('scheduleArrivalTime'):\n",
    "            aschedtime = aschedule.find('scheduleTime')\n",
    "            if aschedtime is not None:\n",
    "                aschedtime = dt.datetime.strptime(aschedtime.text[:-6], format).strftime('%Y%m%d%H%M%S')\n",
    "            else: \n",
    "                aschedtime =''\n",
    "        \n",
    "        # Schreiben in die Datei log_verlauf\n",
    "        #print(operday, journeyOperator, deviceid, lineshortname, ex_lineid, fnr, index, rt, dschedtime, aschedtime, \\\n",
    "        #      dep_del, arr_del, snr, sname, lat, lon, canc, additional, reported_cancelled,ts_reported_cancelled,file=log_verlauf, sep=';')\n",
    "        log_verlauf_arr.append([operday, journeyOperator, deviceid, lineshortname, ex_lineid, \n",
    "                                fnr, index, rt, dschedtime, aschedtime, dep_del, arr_del, snr, sname, lat, lon, canc, additional, ts_reported_cancelled, reported_cancelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort = pd.DataFrame(log_verlauf_arr, columns= ['tag', 'journeyOperator', 'deviceid', 'linie', 'externallinid', 'fahrtnummer', 'index', 'realtime', 'ab_soll', \n",
    "                                  'an_soll', 'ab_delay', 'an_delay', 'stationnumber', 'stationname', 'lat', 'lon', 'cancelled', 'additional', 'ts_reported_cancelled', 'reported_cancelled'])\n",
    "\n",
    "log_sort['index'] = log_sort['index'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rausfiltern der Fahrten ohne Fahrtnummer (aufgetreten 9/2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort = log_sort[~(log_sort.fahrtnummer == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort['fahrtnummer'] = log_sort['fahrtnummer'].astype(np.int32)\n",
    "log_sort['stationnumber'] = log_sort['stationnumber'].astype(np.int32)\n",
    "log_sort['linie'] = log_sort['linie'].astype(str)\n",
    "log_sort['stationname'] = log_sort['stationname'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export als Parquet\n",
    "#log_sort.astype({'linie':'string'}).to_parquet('log/verlauf.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sort['ab_soll'] = log_sort.apply(lambda x: rt_func.format_zeit(x['ab_soll']), axis=1)\n",
    "log_sort['an_soll'] = log_sort.apply(lambda x: rt_func.format_zeit(x['an_soll']), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe der Werte je Fahrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fahrt_array = []\n",
    "root = ET.fromstring(r.text)\n",
    "for child in root.iter('archiveExportJourneyAndDetailsDto'):\n",
    "    for journey in child.iter('journey'):\n",
    "        rt = rt_func.isnone(journey.find('hasRealtime'))\n",
    "        deviceid = rt_func.isnone(journey.find('deviceId'))\n",
    "        fnr = rt_func.isnone(journey.find('journeyID'))\n",
    "        lineshortname = rt_func.isnone(journey.find('lineShortName'))\n",
    "        ex_lineid = rt_func.isnone(journey.find('externalLineId'))\n",
    "        dlid_pre = ex_lineid.split(':')[0:3]\n",
    "        dlid = ':'.join(dlid_pre)\n",
    "        dest = rt_func.isnone(journey.find('destination'))\n",
    "        operday = dt.datetime.strptime(rt_func.isnone(journey.find('operatingDay'))[:-6], format).strftime('%Y-%m-%d')\n",
    "        op = rt_func.isnone(journey.find('journeyOperator'))\n",
    "        cancelled = rt_func.isnone(journey.find('journeyCancelled'))\n",
    "        lastupdate = rt_func.isnone(journey.find('lastJourneyDetailsUpdateTimestamp'))\n",
    "        vehicletypeschedule = rt_func.isnone(journey.find('vehicleTypeSchedule'))\n",
    "        vehicletyperealtime = rt_func.isnone(journey.find('vehicleTypeRealtime'))\n",
    "        ts_reported_cancelled = rt_func.isnone(journey.find('lastTimestampJourneyCancellationReported'))\n",
    "        reported_cancelled = True if len(ts_reported_cancelled) > 0 else False\n",
    "        \n",
    "        journeyRtType = rt_func.isnone(journey.find('journeyRtType'))\n",
    "        try:\n",
    "            lastupdate = dt.datetime.fromtimestamp(int(lastupdate)/1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        except:\n",
    "            continue\n",
    "        distance = rt_func.isnone(journey.find('distanceSchedule'))\n",
    "        \n",
    "        if journey.find('maximumDelay'):\n",
    "            #print(fnr, day)\n",
    "            for maxdelay in journey.iter('maximumDelay'):\n",
    "                #print(maxdelay.find('delay'))\n",
    "                maxd = rt_func.isnone(maxdelay.find('delay'))\n",
    "        else:\n",
    "            maxd=''\n",
    "        \n",
    "        if journey.find('departureDelay'):\n",
    "            #print(fnr, day)\n",
    "            for depdelay in journey.iter('departureDelay'):\n",
    "                ddel = rt_func.isnone(depdelay.find('delay'))\n",
    "        else:\n",
    "            ddel=''\n",
    "        \n",
    "        if journey.find('arrivalDelay'):\n",
    "            #print(fnr, day)\n",
    "            for arrdelay in journey.iter('arrivalDelay'):\n",
    "                adel = rt_func.isnone(arrdelay.find('delay'))\n",
    "        else:\n",
    "            adel=''\n",
    "        \n",
    "        if journey.find('minimumDelay'):\n",
    "            #print(fnr, day)\n",
    "            for mindelay in journey.iter('minimumDelay'):\n",
    "                mind = rt_func.isnone(mindelay.find('delay'))\n",
    "        else:\n",
    "            mind=''\n",
    "        \n",
    "        if journey.find('realArrivalTime'):\n",
    "            #print(fnr, day)\n",
    "            for realarrival in journey.iter('realArrivalTime'):\n",
    "                rat = dt.datetime.strptime(rt_func.isnone(realarrival.find('realTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            rat=''        \n",
    "        \n",
    "        if journey.find('realDepartureTime'):\n",
    "            for realdeparture in journey.iter('realDepartureTime'):\n",
    "                rdt = dt.datetime.strptime(rt_func.isnone(realdeparture.find('realTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            rdt=''\n",
    "\n",
    "        if journey.find('scheduleDepartureTime'):\n",
    "            for realdeparture in journey.iter('scheduleDepartureTime'):\n",
    "                sdt = dt.datetime.strptime(rt_func.isnone(realdeparture.find('scheduleTime'))[:-6], format).strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            sdt=''\n",
    "            \n",
    "        if journey.find('scheduleArrivalTime'):\n",
    "            for realdeparture in journey.iter('scheduleArrivalTime'):\n",
    "                sat = dt.datetime.strptime(rt_func.isnone(realdeparture.find('scheduleTime'))[:-6], format)\\\n",
    "                .strftime('%Y-%m-%d %H:%M:%S')                \n",
    "        else:\n",
    "            sat=''       \n",
    "        \n",
    "        if journey.find('scheduleDepartureStation'):\n",
    "            for dep_station in journey.iter('scheduleDepartureStation'):\n",
    "                sdst = rt_func.isnone(dep_station.find('stationName'))               \n",
    "        else:\n",
    "            sdst=''\n",
    "        \n",
    "        if journey.find('scheduleArrivalStation'):\n",
    "            for arr_station in journey.iter('scheduleArrivalStation'):\n",
    "                sast = rt_func.isnone(arr_station.find('stationName'))               \n",
    "        else:\n",
    "            sast=''\n",
    "        fahrt_array.append([lineshortname, ex_lineid, operday, fnr, rt, op, dest, mind, maxd,  cancelled, lastupdate, \\\n",
    "              distance, sdt, sat, rdt, rat, dlid,sdst, sast,ddel, adel, deviceid, journeyRtType, vehicletypeschedule, vehicletyperealtime, \\\n",
    "                reported_cancelled, ts_reported_cancelled])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.11.0 ('geo_311')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0dcfa513e1bebd633c31802f439dbc67afc7b6eca02dfa4380fc98826b2b9354"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
